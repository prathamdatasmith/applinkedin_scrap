Job Title,Company Name,Location,Job Link,Job Description,Tech Stack
Data engineer II,Swiggy,"Bengaluru, Karnataka, India",https://in.linkedin.com/jobs/view/data-engineer-ii-at-swiggy-4127249617,"Way of working - Remote : Employees will have the freedom to work remotely all through the year. These employees, who form a large majority, will come together in their base location for a week, once every quarter. Job Title : Data Engineer Location : Remote first Tenure : 2-3 years About The Team & Role About Swiggy: Swiggy, founded in 2014, is India's leading tech-driven on-demand delivery platform. With a vision to enhance the urban consumer's quality of life through unparalleled convenience, Swiggy connects millions of consumers with a vast network of restaurants and stores across 500+ cities. Our growth stems from cutting-edge technology, innovative thinking, and well-informed decision-making. Join the Swiggy Data Engineering team to collaborate on decoding hyperlocal trends and impact the entire value chain. Position Overview As a Data Engineer at Swiggy, you will be at the heart of our data-driven approach, collaborating with cross-functional teams to transform raw data into actionable insights. Your role will encompass and Join us as a Data Engineer at Swiggy to contribute significantly to our data ecosystem, drive operational efficiency, and be an integral part of our data-driven journey. Your expertise will play a pivotal role in influencing our strategic decisions and reshaping the food delivery landscape What will you get to do here? Join hands with our Data Engineering team to ensure efficient data collection, storage, and processing. Collaborate in designing and optimizing data pipelines for seamless data movement. Work jointly on data architecture decisions to enhance analytics capabilities. Dive into large, complex datasets to create efficient and optimized queries for analysis. Identify bottlenecks and optimize data processing pipelines for improved performance. Implement best practices for query optimization, ensuring swift data retrieval. Contribute to the DataOps framework, automating data processes and enhancing data quality. Implement monitoring and alerting systems to ensure smooth data operations. Collaborate with the team to develop self-serve platforms for recurring analysis. What qualities are we looking for? Bachelor's or Master’s degree in Engineering, Mathematics, Statistics, or a related quantitative field. 2-4 years of data engineering experience. Proficiency (2-4 years) in SQL, R, Python, Excel, etc., for effective data manipulation. Hands-on experience with Snowflake and Spark/Databricks, adept at Query Profiles and bottleneck identification. Apply creative thinking to solve real-world problems using data-driven insights. Embrace a ""fail fast, learn faster"" approach in a dynamic, fast-paced environment. Exhibit proficient verbal and written communication skills. Thrive in an unstructured environment, demonstrating attention to detail and self-direction. Foster collaboration and partnerships across functions. We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, colour, religion, sex, disability status, or any other characteristic protected by the law. Show more Show less","Data Pipelines, Data Quality, Databricks, Perl, Python, SQL, Snowflake, Spark"
Software Engineer,Microsoft,"Hyderabad, Telangana, India",https://in.linkedin.com/jobs/view/software-engineer-at-microsoft-4077421914,"Microsoft is a company where passionate innovators come to collaborate, envision what can be and take their careers further. This is a world of more possibilities, more innovation, more openness, and the sky is the limit thinking in a cloud-enabled world. Microsoft’s Azure Data engineering team is leading the transformation of analytics in the world of data with products like databases, data integration, big data analytics, messaging & real-time analytics, and business intelligence. The products our portfolio include Microsoft Fabric, Azure SQL DB, Azure Cosmos DB, Azure PostgreSQL, Azure Data Factory, Azure Synapse Analytics, Azure Service Bus, Azure Event Grid, and Power BI. Our mission is to build the data platform for the age of AI, powering a new class of data-first applications and driving a data culture. Within Azure Data, the databases team builds and maintains Microsoft's operational Database systems. We store and manage data in a structured way to enable multitude of applications across various industries. We are on a journey to enable developer friendly, mission-critical, AI enabled operational Databases across relational, non-relational and OSS offerings. We are the team defining the future of the Azure Database for MySQL service, as part of the Azure Data group within the C+AI organization. Azure data teams work at the intersection of cloud, storage, and open-source technologies to solve the pressing challenges of our customers. Our team is building the next generation elastic scale MySQL as a database service that is designed to run planet-scale applications. We work with various open-source components such as Kubernetes, Docker Containers, MySQL etc., and we make regular contributions back. This is your opportunity to be part of a very agile team, take on hard distributed system problems, and ship high impact features at a rapid pace. If you love the engineering challenges of designing and delivering cutting-edge, cloud-scale distributed systems technologies, if you are passionate about developing fault tolerant, performant, manageable, and resilient services that host mission critical customer workloads operated on a massive scale, then this role is for you! We do not just value differences or different perspectives. We seek them out and invite them in so we can tap into the collective power of everyone in the company. As a result, our customers are better served. Responsibilities You will be a key member of the engineering team driving critical engineering design of the product. Some of the core responsibilities include designing, owning and shipping software, writing secure, reliable, scalable and maintainable code. Collaborate with other teams for product features that span across teams and geographies, figuring out dependencies and driving them to completion. You should have a solid understanding of the software development cycle. Successful candidates should have the ability to ramp up quickly on new technologies and adopt solutions within the company or from the Open-Source community. In addition, strong problem solving & debugging skills are necessary. Embody our culture and values Qualifications Required/Minimum Qualifications Bachelor’s degree in Computer Science or Engineering or Mathematics or Physics or IT technical discipline or 2+ years of industry engineering experience 2+ years of programming experience in C# OR C++ OR Java OR object-oriented programming 2+ years of designing the software product. Other Requirements Ability to meet Microsoft, customer and/or government security screening requirements are required for this role. These requirements include, but are not limited to the following specialized security screenings: Microsoft Cloud Background Check: This position will be required to pass the Microsoft Cloud background check upon hire/transfer and every two years thereafter. Preferred/Additional Qualifications Experience building and shipping production grade software or services. Experience building and operating online services and fault-tolerant distributed systems. Experience creating and shipping V1 products using modern development practices. Experience using agile methodologies or test-driven development (TDD). Great curiosity and willingness to question. High enthusiasm, integrity, ingenuity, results-orientation, self-motivation, and resourcefulness in a fast-paced competitive environment. Love the next problem, the next experiment, the next partner. Have a deep desire to work collaboratively, solve problems with groups, find win/win solutions and celebrate successes. Get excited by the challenge of hard technical problems. Solve problems by always leading with deep passion and empathy for customers. Understanding of data structures, algorithms and distributed systems. #azdat #azuredata #azdat #azuredata #mysql #enggjobs Microsoft is an equal opportunity employer. Consistent with applicable law, all qualified applicants will receive consideration for employment without regard to age, ancestry, citizenship, color, family or medical care leave, gender identity or expression, genetic information, immigration status, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran or military status, race, ethnicity, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable local laws, regulations and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application process, read more about requesting accommodations. Show more Show less","Azure Cosmos DB, Azure Data Factory, Azure Synapse, C#, Distributed Systems, Docker, Fabric, Java, Kubernetes, Microsoft Fabric, MySQL, PostgreSQL, Power BI, SQL, SSIS, Scala"
Software Engineer,Microsoft,"Bengaluru, Karnataka, India",https://in.linkedin.com/jobs/view/software-engineer-at-microsoft-4131147250,"Have you ever imagined a world with an infinite amount of storage available and accessible to everyone? A place where everyone in the world can easily access their data from anywhere at any time via any means (e.g., mobile phones, tablets, PCs, smart devices, etc.). Did you ever desire a universally accessible storage system to record all the knowledge known to mankind or to store all the data collected from all the scientists in the world for them to collaborate upon? Do you want to be part of a team that strives to bring these to reality? As a Software Engineer in the Azure Storage team, you will build, improve and support highly scalable, performant services that deliver highly reliable, secure and available access to storage for our customers. You will face challenges of monitoring, analyzing, and designing for ever-growing data needs of our customers and for ensuring data privacy, protection and compliance. This opportunity will allow you to develop your technical skills in cloud services and storage, accelerate career growth, and provide an opportunity to work in a highly dynamic, flexible, and globally distributed team. Microsoft’s mission is to empower every person and every organization on the planet to achieve more. As employees we come together with a growth mindset, innovate to empower others, and collaborate to realize our shared goals. Each day we build on our values of respect, integrity, and accountability to create a culture of inclusion where everyone can thrive at work and beyond. Responsibilities Works with appropriate stakeholders to determine user requirements for a feature. Contributes to the identification of dependencies, and the development of design documents for a product area with little oversight. Creates and implements code for a product, service, or feature, reusing code as applicable. Contributes to efforts to break down larger work items into smaller work items and provides estimation. Acts as a Designated Responsible Individual (DRI) and guides other engineers by developing and following the playbook, working on call to monitor system/product/service for degradation, downtime, or interruptions, alerting stakeholders about status and initiates actions to restore system/product/service for simple and complex problems when appropriate. Proactively seeks new knowledge and adapts to new trends, technical solutions, and patterns that will improve the availability, reliability, efficiency, observability, and performance of products while also driving consistency in monitoring and operations at scale. Qualifications Required Qualifications: Bachelor's Degree in Computer Science, or related technical discipline with proven experience coding in languages including, but not limited to, C, C++, C#, Java, JavaScript, or Python. OR equivalent experience. Excellence in software engineering practices. Other Requirements Ability to meet Microsoft, customer and/or government security screening requirements are required for this role. These requirements include, but are not limited to the following specialized security screenings: Microsoft Cloud Background Check: This position will be required to pass the Microsoft Cloud background check upon hire/transfer and every two years thereafter. Additional / Preferred Qualifications Bachelor's Degree in Computer Science OR related technical field AND 1+ year(s) technical engineering experience with coding in languages including, but not limited to, C, C++, C#, Java, JavaScript, OR Python. OR Master's Degree in Computer Science or related technical field with proven experience coding in languages including, but not limited to, C, C++, C#, Java, JavaScript, or Python OR equivalent experience. Experience developing large-scale , high availability services. Experience working on large-scale automated deployment systems. #Azurecorejobs Microsoft is an equal opportunity employer. Consistent with applicable law, all qualified applicants will receive consideration for employment without regard to age, ancestry, citizenship, color, family or medical care leave, gender identity or expression, genetic information, immigration status, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran or military status, race, ethnicity, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable local laws, regulations and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application process, read more about requesting accommodations. Show more Show less","Azure Storage, C#, Java, JavaScript, Python, SSIS, Scala"
Data Engineer II (Remote),HackerRank,India,https://in.linkedin.com/jobs/view/data-engineer-ii-remote-at-hackerrank-4118673890,"At HackerRank, we help over 2,500 of the most prestigious logos across industries find, hire and upskill amazing developer talent using our SaaS-based Developer Skills Platform. We pioneered and continue to lead the developer skills market. At HackerRank, we are passionate about our mission to "" Change the world to value skills over pedigree” . This position is full-time and remote in India. We are seeking a Data Engineer for Our data team which is on a mission to democratize data at HackerRank, making it accessible and actionable for all. We recently achieved a 10x performance boost with our export service, showcasing our drive for impactful, scalable solutions. With a balanced mix of synchronous and async collaboration, we foster efficiency, inclusivity, and seamless teamwork. What You Will Do Evaluate technologies, develop POCs, solve technical challenges and propose innovative solutions for our technical and business problems Delight our stakeholders, customers and partners by building high-quality, well-tested, scalable and reliable business applications. Design, build and maintain streaming and batch data pipelines that can scale. Architect, develop and maintain our Modern lake house Platform using AWS native infrastructure Designing Complex Data Models to deliver insights and enable self-service Take ownership of scaling, performance, security, and reliability of our data infrastructure Hiring, guiding and mentoring junior engineers Work in an agile development environment, participate in code reviews Collaborate with remote development teams and cross-functional teams What You Will Also Bring 3+ years of experience with designing, developing and maintaining data engineering & BI solutions. Experience with Data Modeling for Big Data Solutions. Experience with Spark, Spark Structured Streaming (Scala Spark) Experience with database technologies like Redshift or Trino Experience with BI Solutions like Looker, Power BI, Amazon Quicksight, Tableau etc is a big plus Experience with ETL Design & Orchestration using platforms like Apache Airflow, MageAI etc is a big plus Experience querying massive datasets using Languages like SQL, Hive, Spark, Trino Experience with performance tuning complex data warehouses and queries. Able to solve problems of scale, performance, security, and reliability Self-driven, initiative taker with good communication skills, ability to lead and mentor junior engineers, work with cross-functional teams, drive architecture decisions Bonus Skills Knowledge of Kafka, Kafka Connect and related technologies is a huge bonus You Will Thrive In This Role, If You love solving tough challenges that create real-world impact and are excited to dive into uncharted territories. You enjoy fast-paced, dynamic environments where collaboration isn’t just encouraged—it’s essential You care about understanding product challenges and finding creative solutions, beyond just coding tasks. You value shipping solutions quickly while refining and enhancing them as you go. You’re willing to break boundaries and contribute wherever needed, even if it’s outside your usual responsibilities. Benefits & Perks One-time home office set up stipend Monthly Remote Work Enablement Stipend Professional Development Reimbursement Wellbeing Benefits (Headspace, etc) Flexible paid time off and paid leave for new parents Insurance for all employees (term life, personal accident, medical) along with medical insurance for their dependents Employee stock options, flexible work hours, and time off About HackerRank HackerRank is a Y Combinator alumnus backed by tier-one Silicon Valley VCs with a total funding of over $100 million. The HackerRank Developer Skills Platform is the standard for assessing developer skills for 2,500+ companies across industries and 26.5M developers worldwide. Companies like LinkedIn, Stripe, and Peloton rely on HackerRank to objectively evaluate skills against millions of developers at every hiring process, allowing teams to hire the best and reduce engineering time. Developers rely on HackerRank to turn their skills into great jobs. We’re data-driven givers who take full ownership of our work and love delighting our customers! HackerRank is a proud equal employment opportunity and affirmative action employer. We provide equal opportunity to everyone for employment based on individual performance and qualification. We never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines. Notice To Prospective HackerRank Job Applicants We’ve noticed fake accounts posing as HackerRank Recruiters on Linkedin and through text. These imposters trick you into paying them for jobs/providing credit check information. Here’s How To Spot The Real Deal Our Recruiters use @hackerrank.com email addresses. We never ask for payment or credit check information to apply, interview, or work here. Thanks for your interest in HackerRank! Show more Show less","Apache Airflow, Data Modeling, Data Pipelines, ETL, Hive, Looker, Power BI, SQL, Scala, Spark, Tableau, Trino"
Senior Data Engineer,Genpact,India,https://in.linkedin.com/jobs/view/senior-data-engineer-at-genpact-4130858844,"Role: Sr/ Lead Databricks Developer (Azure, AWS) Responsibilities Closely work with Architect and lead to design solutions to meet functional and non-functional requirements. Participate to understand architecture and solution design artifacts. Evangelize re-use through the implementation of shared assets. Proactively implement engineering methodologies, standards, and leading practices. Provide insight and direction on roles and responsibilities required for solution operations. Identify, communicate and mitigate Risks, Assumptions, Issues, and Decisions throughout the full lifecycle. Considers the art of the possible, compares various solution options based on feasibility and impact, and proposes actionable plans. Demonstrate strong analytical and technical problem-solving skills. Ability to analyze and operate at various levels of abstraction. Ability to balance what is strategically right with what is practically realistic. Must Have Skills: Must have excellent coding skills either Python or Scala, preferably Python. Must have at least 5+ years of experience in Data Engineering domain with total of 7+ years. Must have implemented at least 2 project end-to-end in Databricks. Must have at least 2+ years of experience on databricks which consists of various components as below Delta lake dbConnect db API 2.0 Databricks workflows orchestration Must be well versed with Databricks Lakehouse concept and its implementation in enterprise environments. Must have strong understanding of Data warehousing and various governance and security standards around Databricks. Must have knowledge of cluster optimization and its integration with various cloud services. Must have good understanding to create complex data pipeline. Must have good knowledge of Data structure & algorithms. Must be strong in SQL and sprak-sql. Must have strong performance optimization skills to improve efficiency and reduce cost. Must have worked on both Batch and streaming data pipeline. Must have extensive knowledge of Spark and Hive data processing framework. Must have worked on any cloud (Azure, AWS, GCP) and most common services like ADLS/S3, ADF/Lambda, CosmosDB/DynamoDB, ASB/SQS, Cloud databases. Must be strong in writing unit test case and integration test. Show more Show less","Data Warehousing, Databricks, Delta Lake, DynamoDB, ELT, Hive, Lakehouse, Python, SQL, Scala, Spark"
Data Engineer,EXL,India,https://in.linkedin.com/jobs/view/data-engineer-at-exl-4128303996,"We are looking for a skilled GCP Data Engineer to design, build, and optimize data pipelines and infrastructure on Google Cloud Platform (GCP). The role involves managing large-scale data processing systems, ensuring data quality, and enabling data-driven decision-making. Key Responsibilities: Design and develop scalable ETL/ELT pipelines on GCP using tools like Dataflow, BigQuery, and Cloud Composer. Build and maintain data architectures, including data lakes and warehouses. Implement data integration and transformation workflows. Optimize data systems for performance, reliability, and cost efficiency. Collaborate with data analysts, scientists, and business teams to meet data requirements. Ensure compliance with security and governance standards. Key Skills: Hands-on experience with GCP services: BigQuery, Dataflow, Pub/Sub, Cloud Storage, etc. Strong expertise in SQL and Python/Java/Scala for data processing. Knowledge of Apache Beam, Airflow, or similar frameworks. Familiarity with CI/CD pipelines and infrastructure-as-code tools (e.g., Terraform). Strong problem-solving and communication skills. This role is perfect for professionals with experience in cloud data engineering who are passionate about building robust and efficient data systems. Show more Show less","Apache Beam, BigQuery, Cloud Storage, Data Lake, Data Pipelines, Data Quality, ELT, ETL, Java, Python, SQL, Scala, Terraform"
Data Engineer,EXL,India,https://in.linkedin.com/jobs/view/data-engineer-at-exl-4128306543,"We are looking for a skilled GCP Data Engineer to design, build, and optimize data pipelines and infrastructure on Google Cloud Platform (GCP). The role involves managing large-scale data processing systems, ensuring data quality, and enabling data-driven decision-making. Key Responsibilities: Design and develop scalable ETL/ELT pipelines on GCP using tools like Dataflow, BigQuery, and Cloud Composer. Build and maintain data architectures, including data lakes and warehouses. Implement data integration and transformation workflows. Optimize data systems for performance, reliability, and cost efficiency. Collaborate with data analysts, scientists, and business teams to meet data requirements. Ensure compliance with security and governance standards. Key Skills: Hands-on experience with GCP services: BigQuery, Dataflow, Pub/Sub, Cloud Storage, etc. Strong expertise in SQL and Python/Java/Scala for data processing. Knowledge of Apache Beam, Airflow, or similar frameworks. Familiarity with CI/CD pipelines and infrastructure-as-code tools (e.g., Terraform). Strong problem-solving and communication skills. This role is perfect for professionals with experience in cloud data engineering who are passionate about building robust and efficient data systems. Show more Show less","Apache Beam, BigQuery, Cloud Storage, Data Lake, Data Pipelines, Data Quality, ELT, ETL, Java, Python, SQL, Scala, Terraform"
Senior Data Engineer,Ticketmaster,"Gurugram, Haryana, India",https://in.linkedin.com/jobs/view/senior-data-engineer-at-ticketmaster-4122805024,"WHAT THIS ROLE WILL DO This is a hands-on Data role with strong Python/PySpark coding skills Contribute to the enhancements and continuous improvement of performance and reliability of the existing Data Platform Services to meet the requirements from Data Engineering/Product team Design and build self-servicing onboarding and automation capabilities in Core Data Platform services in a scalable, reliable and secure way. Design and build platform capabilities to support onboarding and operation effectiveness Design and develop scalable data platform and integration solutions for various data sources leveraging Databricks Unified Platform Ensure data quality and integrity across all data systems. Develop and maintain documentation for data systems and processes. Monitor and troubleshoot data platform issues. Participate in on-call rotations/Pagerduty for data platform support. TECHNICAL SKILLS/COMPETENCIES Good understanding of Data Lakes, Data Warehouses is MUST to have Software development, coding expertise in the Data Engineering field using any of the Python/Pandas/PySpark is must to have Hands on experience using version control systems such as Git and CI/CD workflows and practice Design and Develop data ingestion services that are highly performant, reliable and scalable Decent expertise using ANSI SQL and Spark SQL is key to have Workflow automation, orchestration using Airflow or equivalent tools stack Hands on working knowledge on at least in one of these: Databricks, Hadoop and related stacks Working experience in at least one of cloud services from any of the Amazon AWS, Google GCP or Microsoft Azure, preferably AWS streaming (Kafka) and batch-based data sources diverse data sources and data formats (xml, json, yaml, parquet, avro, delta) and respective use cases Any visualization experience is an advantage so you can bring clarity to prod incidents by using them is a nice to have An excellent understanding of the nuances that add complexities around time zones, geo, various data formats, data types across different storage systems is nice to have Agile development methodologies using the Atlassian suite: Jira, Confluence Collaborate with cross-functional teams to deliver data solutions and provide technical support and guidance to team members. Stay up to date with the latest data engineering technologies and trends. Show more Show less","Data Lake, Data Quality, Databricks, ELT, Pyspark, Python, SQL, Scala, Spark"
Data Engineer - Python + Snowflake,Smart Lion Private Limited,"Ahmedabad, Gujarat, India",https://in.linkedin.com/jobs/view/data-engineer-python-%2B-snowflake-at-smart-lion-private-limited-4119250076,"Position: Data Engineer - Python + Snowflake Location: 100% Remote Salary: up to 25 LPA Responsibilities Drive products through the development lifecycle, from proof of concept to production-ready systems. Engineer, build, and maintain scalable automated data pipelines. Apply best practices in data management, including processing, quality, and lineage. Manage code repositories and deploy code using GIT. Automate and maintain system operations to ensure product availability, performance, and scalability. Support regular and ad-hoc data querying and analysis. Requirements 3-5 years of experience in Python and data-focused libraries such as Pandas and Numpy, with experience in handling JSON, XML, CSV, and TSV formatted data. 3-5 years of experience with Snowflake and SQL. 2-3 years of experience using version control tools like Git, Bitbucket, and working in a Linux environment. Familiarity with API consumption is a plus. Experience with Java, Tableau, and Agile practices is beneficial. Knowledge of the financial domain is a plus. Benefits Dedicated Learning & Development Budget : Invest in your growth with a budget specifically allocated for learning. Conference Talks Sponsorship : Get fully sponsored and supported if you're speaking at a conference. Cutting-Edge Projects : Work on innovative projects involving the latest in AI technology. Employee-Friendly Leave Policy : Enjoy a variety of leave options to maintain a healthy work-life balance. Comprehensive Insurance : Medical and term life insurance for you and your family’s peace of mind. And More : Additional perks to support your well-being and career development. Show more Show less","Data Pipelines, Java, Python, SQL, Scala, Snowflake, Tableau"
Data Engineer (Bengaluru),Demyst,"Bengaluru, Karnataka, India",https://in.linkedin.com/jobs/view/data-engineer-bengaluru-at-demyst-3621008248,"Our Solution Demyst unlocks innovation with the power of data. Our platform helps enterprises solve strategic use cases, including lending, risk, digital origination, and automation, by harnessing the power and agility of the external data universe. We are known for harnessing rich, relevant, integrated, linked data to deliver real value in production. We operate as a distributed team across the globe and serve over 50 clients as a strategic external data partner. Frictionless external data adoption within digitally advancing enterprises is unlocking market growth and allowing solutions to finally get out of the lab. If you like actually to get things done and deployed, Demyst is your new home. The Opportunity As a Data Engineer at Demyst, you will be powering the latest technology at leading financial institutions around the world. You may be solving a fintech's fraud problems or crafting a Fortune 500 insurer's marketing campaigns. Using innovative data sets and Demyst's software architecture, you will use your expertise and creativity to build best-in-class solutions. You will see projects through from start to finish, assisting in every stage from testing to integration. To meet these challenges, you will access data using Demyst's proprietary Python library via our JupyterHub servers, and utilize our cloud infrastructure built on AWS, including Athena, Lambda, EMR, EC2, S3, and other products. For analysis, you will leverage AutoML tools, and for enterprise data delivery, you'll work with our clients' data warehouse solutions like Snowflake, DataBricks, and more. While this is a fully-remote role, successful candidates are expected to be based to Bengaluru where Demyst is building its next hub. As you will be supporting clients globally, you will be expected to align your working hours to the US. Responsibilities Collaborate with internal project managers, sales directors, account managers, and clients' stakeholders to identify requirements and build external data-driven solutions Perform data appends, extracts, and analyses to deliver curated datasets and insights to clients to help achieve their business objectives Understand and keep current with external data landscapes such as consumer, business, and property data. Engage in projects involving entity detection, record linking, and data modelling projects Design scalable code blocks using Demyst's APIs/SDKs that can be leveraged across production projects Govern releases, change management and maintenance of production solutions in close coordination with clients' IT teams Requirements Bachelor's in Computer Science, Data Science, Engineering or similar technical discipline (or commensurate work experience); Master's degree preferred 1-3 years of Python programming (with Pandas experience) Experience with CSV, JSON, parquet, Avro, and other common formats Data cleaning and structuring (ETL experience) Knowledge of API (REST and SOAP), HTTP protocols, API Security and best practices Experience with SQL, Git, and Airflow Strong written and oral communication skills Excellent attention to detail Ability to learn and adapt quickly Willingness to align to US working hours Answering the screening questions: all applicants who do not provide thoughtful and detailed answers to the two questions provided in the application form will be automatically disqualified Benefits Distributed working team and culture Generous benefits and competitive compensation Collaborative, inclusive work culture: all-company offsites and local get togethers in Bangalore Annual learning allowance Office setup allowance Generous paid parental leave Be a part of the exploding external data ecosystem Join an established fast growth data technology business Work with the largest consumer and business external data market in an emerging industry that is fueling AI globally Outsized impact in a small but rapidly growing team offering real autonomy and responsibility for client outcomes Stretch yourself to help define and support something entirely new that will impact billions Work within a strong, tight-knit team of subject matter experts Small enough where you matter, big enough to have the support to deliver what you promise Transfer to US and Australia offices possible for top performers after two years Demyst is committed to creating a diverse, rewarding career environment and is proud to be an equal opportunity employer. We strongly encourage individuals from all walks of life to apply. Show more Show less","Databricks, ETL, Python, SQL, SSIS, Scala, Snowflake"
Data Warehouse Engineer / Developer,Forbes Advisor,"Chennai, Tamil Nadu, India",https://in.linkedin.com/jobs/view/data-warehouse-engineer-developer-at-forbes-advisor-4114929611,"Company Description Forbes Advisor is a new initiative for consumers under the Forbes Marketplace umbrella that provides journalist- and expert-written insights, news and reviews on all things personal finance, health, business, and everyday life decisions. We do this by providing consumers with the knowledge and research they need to make informed decisions they can feel confident in, so they can get back to doing the things they care about most. If you're looking for challenges and opportunities similar to those of a startup, with the benefits of a seasoned and successful company, then read on: Job Description Roles and Responsibilities: Develop and Design DW Solutions as per industry standards, practices and procedures and attributes of the business intelligence tools and systems. Work on multiple projects as a team member. Develop Data Load jobs / Scripts / Sequence, Reviews and validates data loaded into the data warehouse for accuracy. Responsible for prototyping solutions, preparing test scripts, and conducting tests and data replication, extraction, loading, cleansing, and data modeling for data warehouses. Implement Best Practices in data warehouses and advise users on conflicts and inappropriate data usage. Understand and implement data warehouse development in multi-platform environments. Understand data mining in a data warehouse environment which includes data design, database architecture, metadata, and repository creation. Maintain knowledge of software tools, languages, scripts, and shells that effectively support the data warehouse environment in different operating system environments Requirements: Independently design and implement Data Warehousing solutions, ETL , BI ,Database Good knowledge in DBT. BE / B .Tech / MCA Good to have : Strong verbal and written communications, team collaboration, time management. Primary Skills: Data Warehousing, ETL , BI exposure ,Database Experience in Data Warehouse: 5 to 8 years. Experience in Data Analytics: 2 years Experience in ETL: 3 to 5 years (DBT is an advantage or Informatica, DataStage, , Azure etc.,) Experience in Data Modeling: basic understand Physical and logical models Experience in SQL & Shell Script - Basic knowledge is must Perks: Day off on the 3rd Friday of every month (one long weekend each month) Monthly Wellness Reimbursement Program to promote health well-being Paid paternity and maternity leaves Qualifications Strong exp in DBT. Show more Show less","DBT, Data Modeling, Data Replication, Data Warehousing, ETL, Informatica, SQL"
Data Engineer,Concentrix,"Hyderabad, Telangana, India",https://in.linkedin.com/jobs/view/data-engineer-at-concentrix-4123855186,"Job Title: Data Engineer Job Description We’re Concentrix. A new breed of tech company — Human-centered. Tech-powered. Intelligence-fueled. We create game-changing solutions across the enterprise, that help brands grow across the world and into the future. We are trusted by clients across all major sectors, from up-and-coming success stories to iconic Fortune Global 500 brands in over 70 countries spanning 6 continents. Our game-changers: Challenge Conventions Deliver outcomes unimagined Create experiences that go beyond WOW If this is you, we would love to discuss career opportunities with you. In our Concentrix Catalyst team, you will work with the engine that powers the experience design and engineering capabilities at Concentrix. A leading global solutions company that reimagines everything CX through strategy, talent, and technology. We combine human-centered design, powerful data, and strong tech to accelerate CX transformation at scale. You will be surrounded by the best in the world providing market leading technology and insights to modernize and simplify the customer experience. Within our professional services team, you will deliver strategic consulting, design, advisory services, market research, and contact center analytics that deliver insights to improve outcomes and value for our clients. Hence achieving our vision. Concentrix provides eligible employees with an opportunity to enroll in many benefit programs, generally including private medical plans, great compensation package, retirement savings plans, paid learning days, and flexible workplaces. Specific benefits plans will vary by country/region. We’re a remote-first company looking for the absolute best talent in the world. Experience the power of a game-changing career. Job Description Key Responsibilities: Data Pipeline Development: Design and implement scalable data pipelines to collect, process, and store data for analytic use. Ensure data integrity and consistent data flow across various sources and systems. Automate data loading and transformations using ETL tools and frameworks. Data Architecture & Infrastructure: Develop and optimize data storage solutions, including databases, data warehouses, and data lakes. Implement robust data models and adhere to best practices in data architecture. Collaborate with IT teams to ensure seamless integration and deployment. Collaboration & Support: Work closely with data analysts, data scientists, and business stakeholders to understand data needs. Provide support for data access, data visualization, and reporting tools. Troubleshoot and resolve data-related technical issues swiftly and effectively. Security & Compliance: Ensure all data processes comply with applicable data privacy laws and corporate policies. Implement security measures to protect data assets and prevent unauthorized access. Continuous Improvement: Evaluate new data engineering tools and technologies to enhance data processing capabilities. Proactively identify opportunities to improve data infrastructure and processes. Qualifications: Bachelor’s degree in Computer Science, Engineering, Data Science, or a related field; Master’s degree preferred. Minimum of 3 years of experience in data engineering, data warehousing, or a similar role. Proficiency in programming languages such as Python, Java, or Scala. Experience with ETL tools and cloud platforms (e.g., AWS, Azure, Google Cloud). Strong understanding of SQL and database design concepts. Excellent problem-solving skills and attention to detail. Location: IND Hyderabad Hitech Work-at-Home Language Requirements: Time Type: Full time If you are a California resident, by submitting your information, you acknowledge that you have read and have access to the Job Applicant Privacy Notice for California Residents R1570395 Show more Show less","Data Lake, Data Pipelines, Data Warehousing, ETL, Java, Python, SQL, Scala"
Senior Data Engineer,Cummins Inc.,India,https://in.linkedin.com/jobs/view/senior-data-engineer-at-cummins-inc-4132203197,"Key Responsibilities: Lead Data Engineering Projects: Oversee the development and deployment of end-to-end data ingestion pipelines using Azure Databricks, Apache Spark, and related technologies, ensuring scalability, performance, and efficiency. Design & Architecture: Design high-performance, resilient, and scalable data architectures for data ingestion and processing using best practices for Azure Databricks and Spark. Team Leadership: Provide technical guidance and mentorship to a team of data engineers, fostering a culture of collaboration, continuous learning, and innovation. Collaboration: Work closely with data scientists, business analysts, and other stakeholders to understand data requirements and ensure smooth integration of various data sources into the data lake/warehouse. Optimization & Performance Tuning: Ensure data pipelines are optimized for speed, reliability, and cost efficiency in an Azure environment. Conduct performance tuning, troubleshooting, and debugging of Spark jobs and Databricks clusters. Code Quality & Best Practices: Enforce and advocate for best practices in coding standards, version control, testing, and documentation. Integration with Azure Services: Work with other Azure services such as Azure Data Lake Storage, Azure SQL Data Warehouse, Azure Synapse Analytics, and Azure Blob Storage to integrate data seamlessly. Continuous Improvement: Stay current with industry trends and emerging technologies in the field of data engineering and make recommendations for improvements to the team’s tools and processes. Ensure Data Quality: Implement data validation and data quality checks as part of the data ingestion process to ensure consistency, accuracy, and integrity of ingested data. Risk & Issue Management: Proactively identify risks and blockers and resolve complex technical issues in a timely and effective manner. Qualifications: Education: Bachelor’s or master’s degree in computer science, Information Technology, Engineering, or a related field. Experience: 8+ years of experience in data engineering or a related field. Strong hands-on experience with Azure Databricks , Spark , Python/Scala, CICD, Scripting for data processing and snowflake Experience working in multiple file formats like Parquet , Delta , and Iceberg . Knowledge of Kafka or similar streaming technologies for real-time data ingestion. Experience with data governance and data security in Azure. Proven track record of building large-scale data ingestion and ETL pipelines in cloud environments, specifically Azure. Deep understanding of Azure Data Services (e.g., Azure Blob Storage, Azure Data Lake, Azure SQL Data Warehouse, Event Hubs, Functions etc.). Familiarity with data lakes , data warehouses , and modern data architectures. Experience with CI/CD pipelines , version control (Git), Jenkins and agile methodologies. Understanding of cloud infrastructure and architecture principles (especially within Azure ). Technical Skills: Expert-level proficiency in Spark, SPARK Streaming , including optimization, debugging, and troubleshooting Spark jobs. Solid knowledge of Azure Databricks for scalable, distributed data processing. Strong coding skills in Python and Scala for data processing. Experience working with SQL , especially for large datasets. Knowledge of data formats like Iceberg , Parquet , ORC , and Delta Lake . Leadership Skills: Proven ability to lead and mentor a team of data engineers, ensuring adherence to best practices. Excellent communication skills, capable of interacting with both technical and non-technical stakeholders. Strong problem-solving, analytical, and troubleshooting abilities. Soft Skills: Excellent communication and interpersonal skills. Strong organizational skills with the ability to manage multiple tasks and priorities. Ability to work in a fast-paced, constantly evolving environment. Strong collaborative skills and ability to work effectively across teams. Show more Show less","Azure Data Lake, Azure Databricks, Azure Synapse, Data Governance, Data Lake, Data Pipelines, Data Quality, Databricks, Delta Lake, ELT, ETL, Jenkins, Python, SQL, Scala, Snowflake, Spark"
Software Engineer,Lumenci,India,https://in.linkedin.com/jobs/view/software-engineer-at-lumenci-4050526543,"Responsibilities And Duties Design, develop, and implement scalable and efficient software tools related to AI-powered Lumenci's Platforms, integrating with data pipelines, machine learning models, and data visualization tools. Collaborate with cross-functional teams, including engineering, sales, marketing, product managers, and other stakeholders, to understand business requirements, develop SaaS products, and deliver a smooth user experience with high-end look and feel. Contribute to building a robust product architecture, follow best practices in coding and code reviews, and be familiar with test-driven development. Develop dashboard tools to automate manual processes, build and maintain easy-to-use and powerful web applications. Develop and optimize algorithms for data processing, feature engineering, and model training. Participate in the full software development lifecycle, including requirements gathering, design, coding, testing, deployment, and maintenance. Research and implement state-of-the-art NLP models and techniques to improve the accuracy and performance of AI-powered solutions. Collaborate with the team to develop and maintain reusable NLP components, libraries, and frameworks that can be integrated into various software applications. Participate in the evaluation and selection of NLP tools, libraries, and frameworks that align with the team's goals and requirements. Work with team to ensure high-quality training data for NLP models and provide guidance on annotation best practices and guidelines. Optimize NLP models for scalability, performance, and resource utilization to ensure efficient deployment in production environments. Continuously monitor and maintain deployed NLP models, ensuring their performance, accuracy, and reliability over time. Ensure the quality, performance, and reliability of data-driven software solutions through rigorous testing, debugging, and code reviews. Stay up to date with the latest trends, techniques, and best practices in data science, machine learning, and software engineering. Contribute to the continuous improvement of software development processes, tools, and methodologies. Follow Agile/Scrum methodologies to deliver reliable software through continuous integration and continuous delivery. Expected Competencies And Skills Strong programming skills in Python or C++. Exposure to the latest technologies in AI, ML, and Blockchain. Proficiency in data manipulation, analysis, and visualization using libraries like pandas, NumPy, and Matplotlib. Experience with machine learning frameworks such as scikit-learn, TensorFlow, or PyTorch. Familiarity with databases (e.g., SQL, NoSQL) and data storage technologies. Experience building web applications from scratch. Exposure to web frameworks such as Django. Familiarity with third-party APIs such as Stripe and Sendgrid. Knowledge of cloud computing platforms like AWS, GCP, or Azure. Exposure to DevOps practices and tools (e.g., Docker, Kubernetes). Familiarity with large language models (LLMs) and experience with frameworks like langchain, huggingface, pinecone, openai-api, etc. Understanding of linguistics and language structures. Knowledge of natural language processing (NLP) techniques, such as text classification, named entity recognition, and topic modeling. Experience with text data preprocessing, including tokenization, stemming, and lemmatization. Knowledge of evaluation metrics for NLP tasks (e.g., accuracy, precision, recall, F1-score). Experience with pre-trained language models like BERT, GPT, etc. Familiarity with NLP libraries and tools such as NLTK, spaCy, etc is a plus. Experience with deploying NLP models in production environments. Continuous learning and staying up-to-date with the latest research and advancements in the NLP field. Knowledge of software development best practices, including version control (e.g., Git), testing, and documentation. Excellent problem-solving and analytical skills. Strong communication and collaboration abilities. Education And Experience BTech/ MTech in Computer Science & Engineering 3-5 years of relevant experience in software development as individual contributor Benefits Performance-driven compensation package. Rapid career growth. An ideal candidate would share our way of working. Solve for the Customer: Lumenci is a customer first company, with the focus to create a long-term relationship with our clients. Customer here includes internal employees and candidates who are part of the recruitment process. Quality, Ownership and Accountability: We are passionate about results and take full ownership of our work. We are performance oriented and have a drive for excellence. Collaboration: We encourage collaboration over competition, work in small teams and believe that teams do better than individuals. Growth Mindset: We are adaptable to changing requirements and needs of a dynamic high growth company. We encourage each other to take diverse initiatives and develop new competencies. About Lumenci Lumenci is the technology industry’s most strategic patent monetization partner. We work with the world’s top technology companies, law firms, inventors, and start-ups to find the value in their inventions and help them pursue—and defend—that value throughout the ideation to monetization lifecycle. We help clients convert innovation into patent portfolios and identify their best monetization opportunities. We work with a wide variety of technologies including hardware and software, telecom, networking, and biotech technologies. Lumenci combines technology domain expertise with strategic industry connections to guide towards best route to ROI. From ideation to monetization -- we illuminate the way. Skills: aws,azure,software development,lemmatization,topic modeling,natural language processing,docker,artificial intelligence,python,ml,gcp,kubernetes,tokenization,pytorch,ai,nltk,django,data manipulation,tensorflow,gpt,backend development,scikit-learn,stemming,blockchain,nlp,numpy,sql,named entity recognition,matplotlib,pandas,c++,nosql,git,devops,text classification,machine learning,bert,spacy Show more Show less","Data Pipelines, Docker, Kubernetes, LangChain, Machine Learning, NLTK, Python, SQL, Scala, Scikit-Learn, spaCy"
Data Engineer,Forbes Advisor,"Chennai, Tamil Nadu, India",https://in.linkedin.com/jobs/view/data-engineer-at-forbes-advisor-4137254913,"Company Description Forbes Advisor is a new initiative for consumers under the Forbes Marketplace umbrella that provides journalist- and expert-written insights, news and reviews on all things personal finance, health, business, and everyday life decisions. We do this by providing consumers with the knowledge and research they need to make informed decisions they can feel confident in, so they can get back to doing the things they care about most. Forbes Advisor is looking to hire a Data Engineer to perform and Build core data products, Promote data-driven culture, Democratize insights through self-service, and Establish a single source of truth in business and customer metrics If you're looking for challenges and opportunities similar to those of a startup, with the benefits of a seasoned and successful company, then read on: Job Description Responsibilities: Assemble large, complex data that meet functional/non-functional business requirements. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing transformation for greater scalability, etc. Use the infrastructure/services required for optimal extraction, transformation, and loading of data from a wide variety of data sources using GCP and AWS services. Create data for analytics and CRM team members that assist them in building and optimizing our product. Work with stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support their data requirement needs. Requirements: Bachelor’s degree with Minimum 5+ years of experience working in globally distributed teams successfully Must have experience working on Spark, Kafka, and Python Apply experience with cloud storage and computing for data pipelines in GCP (GCS, BQ, composer, etc) Write pipelines in Airflow to orchestrate data pipelines Experience handling data from 3rd party providers: Google Analytics, Google Ads etc. Strong analytic skills related to working with unstructured datasets. Experience in manipulating, processing and extracting value from large disconnected datasets. Experience with software engineering practices in data engineering, e.g. release management, testing, etc and corresponding tooling (dbt, great expectations, …) Basic knowledge on dbt Experience with data governance, privacy and security Basic ML exposure & know-how supporting teams as they migrate through guidelines, support and adherence to good practices. Dynamically adapt to fluid situations to deliver the intended results Strong interpersonal and conflict resolution skills Ability to form productive relationships quickly and create the spheres of influence required for transformational changes Excellent verbal and written communication skills Perks: Day off on the 3rd Friday of every month (one long weekend each month) Monthly Wellness Reimbursement Program to promote health well-being Paid paternity and maternity leaves Responsibilities: Assemble large, complex data that meet functional/non-functional business requirements. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing transformation for greater scalability, etc. Use the infrastructure/services required for optimal extraction, transformation, and loading of data from a wide variety of data sources using GCP and AWS services. Create data for analytics and CRM team members that assist them in building and optimizing our product. Work with stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support their data requirement needs. Requirements: Bachelor’s degree with Minimum 5+ years of experience working in globally distributed teams successfully Must have experience working on Spark, Kafka, and Python Apply experience with cloud storage and computing for data pipelines in GCP (GCS, BQ, composer, etc) Write pipelines in Airflow to orchestrate data pipelines Experience handling data from 3rd party providers: Google Analytics, Google Ads etc. Strong analytic skills related to working with unstructured datasets. Experience in manipulating, processing and extracting value from large disconnected datasets. Experience with software engineering practices in data engineering, e.g. release management, testing, etc and corresponding tooling (dbt, great expectations, …) Basic knowledge on dbt Experience with data governance, privacy and security Basic ML exposure & know-how supporting teams as they migrate through guidelines, support and adherence to good practices. Dynamically adapt to fluid situations to deliver the intended results Strong interpersonal and conflict resolution skills Ability to form productive relationships quickly and create the spheres of influence required for transformational changes Excellent verbal and written communication skills Perks: Day off on the 3rd Friday of every month (one long weekend each month) Monthly Wellness Reimbursement Program to promote health well-being Paid paternity and maternity leaves Additional Information All your information will be kept confidential according to EEO guidelines. Show more Show less","Cloud Storage, DBT, Data Governance, Data Pipelines, Python, SSIS, Scala, Spark"
Software Engineer,Incubyte,"Pune, Maharashtra, India",https://in.linkedin.com/jobs/view/software-engineer-at-incubyte-4136013992,"Skills: Software Development, Object-Oriented Design, Version Control, Ruby, Ruby On Rails, Test-Driven Development, Clean Coding, Model-View-Controller (MVC), Job Description Company Description Incubyte is a software engineering company that incubates high-performing software engineering teams for clients. Our goal is to create and release products that are relevant today and in the future. We offer services to quickly launch MVPs, to help scale-up existing products, bring stability and predictability to legacy software, and optimize manual processes. We are a team of Software Craftspeople who embrace eXtreme Programming Practices and believe in total ownership of the SDLC end-to-end. One of our core values is continuous learning & mastery and we strive to find passionate individuals who are eager to grow and succeed with us. Role Description This is a full-time remote role for a Ruby on Rails Developer. As a Ruby on Rails Developer, your day-to-day tasks will involve back-end web development, front-end development, and implementing object-oriented programming (OOP) principles. You will follow test-driven development practices, and add an element of craft to your code. You will collaborate with cross-functional teams to develop and improve software solutions to provide value to your clients on a regular basis. Qualifications Back-End Web Development and Front-End Development skills Strong understanding of Object-Oriented Programming (OOP) principles Experience working within the Model-View-Controller (MVC) framework Familiarity with Test-Driven Development practices Experience with Ruby on Rails Knowledge of HTML, CSS, and JavaScript Excellent problem-solving and debugging skills Strong communication and collaboration abilities Ability to work independently and remotely Experience with agile development methodologies is a plus Bachelor's degree in Computer Science or related field is preferred Show more Show less","Java, JavaScript"
Data Engineer,Recro,India,https://in.linkedin.com/jobs/view/data-engineer-at-recro-4124987850,"Experience: 3+ years Responsibilities: Design, develop, and maintain scalable data pipelines and systems. Implement data integration solutions using Azure Data Factory. Develop and manage data lakes and Azure Data Lake Storage. Ensure data quality and implement data governance policies. Collaborate with data scientists and analysts to provide data solutions. Optimize and manage SQL databases and data warehouses. Integrate Databricks for advanced analytics and data processing. Write complex SQL queries, including joins and indexes, to extract and manipulate data. Required Skills: Azure Data Factory: Experience in building and managing data pipelines. Azure Data Lake Storage : Proficiency in managing large-scale data storage. Databricks: Hands-on experience with data processing and analytics. Microsoft SQL Server: Strong skills in SQL database management. SQL & Python & Pyspark : Expertise in writing and optimizing complex queries. Data Governance: Ensuring data quality and compliance with policies. Performance Tuning: Optimizing data systems for performance and scalability. Collaboration: Working with cross-functional teams to deliver data solutions. Show more Show less","Azure Data Factory, Azure Data Lake, Data Governance, Data Lake, Data Pipelines, Data Quality, Databricks, Pyspark, Python, SQL, SQL Server, Scala, Spark"
Data Engineer,System Soft Technologies,India,https://in.linkedin.com/jobs/view/data-engineer-at-system-soft-technologies-4116573599,"Job Description – Data Engineer System Soft Technologies is a premier technology company providing exceptional consulting services and solutions that drive innovation, enhance business value, and boost competitiveness. For over 25 years, we have built trusted partnerships with our clients, helping us grow into a $200MM+ enterprise. With the collective resources of over 1,200 associates covering the full spectrum of IT expertise, we anticipate and meet the unique needs of our clients, consistently delivering exceptional quality that exceeds expectations. Why System Soft Technologies? At System Soft Technologies, we are united by diversity, inclusion, transparency, respect, integrity, and a deep passion for our clients and team members. Your professional development fuels our growth as we collaborate, share ideas, innovate, and invest in our future together. By building meaningful partnerships with our people, we embrace a common purpose to amplify ambitions, expand horizons, and exceed our goals. This synergy keeps us agile, ahead of the competition, and leaders in our industry. Our continued success starts with you. Job Description: They must have experience with Python, Snowflake and Alteryx. This person will be working in India-standard time hours to cover the time when the US Teams’ are not available. Media experience is nice to have but do prefer at least they come from product-centric companies. Responsibilities: Quality of having a two-person team. Transforming data into a model consumed by Tableau. Ensuring the data model supports application changes. Responsible for pipelines, not Tableau. Problem-solving is key, with a focus on contextual awareness. Brand-new project and solving new problems. Qualifications: Python Snowflake Alteryx Knowledge of AWS Managing access issues via AWS Console Provisions S3 (storage layer) System Soft Technologies is a proud equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, national origin, disability status, sexual orientation, or any other characteristic protected by law. Show more Show less","Python, Snowflake, Tableau"
Data Engineer,Recro,India,https://in.linkedin.com/jobs/view/data-engineer-at-recro-4124899256,"Role- Azure Data Engineer Notice Period- Immediate/15 Days Location- Remote Experience- 3+ Years We are looking for a skilled Azure Data Engineer with expertise in ETL processes and Python . Key Highlights of the Role: Design and maintain scalable ETL pipelines in the Azure ecosystem. Leverage tools like Azure Data Factory , Databricks , and Synapse Analytics . Develop and optimize data workflows using Python . Ensure data quality, integrity, and performance. Collaborate with cross-functional teams to deliver impactful data solutions. Experience with SQL , data modeling, and schema design required. Bonus: Azure certifications like Azure Data Engineer Associate are a plus! Show more Show less","Azure Data Factory, Data Modeling, Data Quality, Databricks, ETL, Python, SQL, Scala, Schema Design"
Data Engineer,Baker Hughes,"Bengaluru, Karnataka, India",https://in.linkedin.com/jobs/view/data-engineer-at-baker-hughes-4136595404,"Are you passionate to work with the team to create state-of-the-art data and analytics driven solutions? Are you interested in driving business analytics to a new level of predictive analytics while leveraging big data tools and technologies. Join our Data Engineering Team! The Data Engineering team helps solve our customers' toughest challenges; making flights safer, power cheaper, and oil & gas production safer for people and the environment by leveraging data and analytics. The Lead Data Engineer will work with the team to create state-of-the-art data and analytics driven solutions, working across Baker Hughes to drive business analytics to a new level of predictive analytics while leveraging big data tools and technologies. Partner with the best As a Data Engineer, you will be part of a data engineering or cross-disciplinary team on commercially-facing development projects, typically involving large, complex data sets. These teams typically include statisticians, computer scientists, software developers, engineers, product managers, and end users, working in concert with partners in Baker Hughes business units. Potential application areas include remote monitoring and diagnostics across infrastructure and industrial sectors, financial portfolio risk assessment, and operations optimization. As a Data Engineer, you will be responsible for: Performing a variety of data loads & data transformations. Working knowledge of methods for parsing, formatting, & transforming data into units consistent with analytical needs. Demonstrating proficiency in implementation of logical/physical data models that support MDM best practices. Performing integration of multiple data source-formats into master data load. Having Proficiency in the use of at least one ETL tool. Having good communication skills - both oral and written Identifying downstream implications of data loads/migration (e.g., data quality, regulatory, etc). Demonstrating skills in collaborating with different stakeholders, influencing decisions and an ability to take strategic view Fuel your passion To be successful in this role you will: Have a Bachelors Degree with minimum of 2 years of working experience. Have hands on experience working with database technologies, including ETL tools including Databricks Workflows using Pyspark / Python, and an ability to learn new technologies. Have good proficiency in writing and optimizing SQL queries and working with databases. Translates analytics problems into data requirements. Understands logical and physical data models, big data storage architecture, data modeling methodologies, metadata management, master data management, data lineage & data profiling. Understands the technology landscape, up to date on current technology trends and new technology, brings new ideas to the team. Work in a way that works for you We recognize that everyone is different and that the way in which people want to work and deliver at their best is different for everyone too. In this role, we can offer the following flexible working patterns: Working remotely from home or any other work location Working with us Our people are at the heart of what we do at Baker Hughes. We know we are better when all of our people are developed, engaged and able to bring their whole authentic selves to work. We invest in the health and well-being of our workforce, train and reward talent and develop leaders at all levels to bring out the best in each other. Working for you Our inventions have revolutionized energy for over a century. But to keep going forward tomorrow, we know we have to push the boundaries today. We prioritize rewarding those who embrace change with a package that reflects how much we value their input. Join us, and you can expect: Contemporary work-life balance policies and wellbeing activities Comprehensive private medical care options Safety net of life insurance and disability programs Tailored financial programs Additional elected or voluntary benefits About Us: We are an energy technology company that provides solutions to energy and industrial customers worldwide. Built on a century of experience and conducting business in over 120 countries, our innovative technologies and services are taking energy forward – making it safer, cleaner and more efficient for people and the planet. Join Us: Are you seeking an opportunity to make a real difference in a company that values innovation and progress? Join us and become part of a team of people who will challenge and inspire you! Let’s come together and take energy forward. Baker Hughes Company is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law. R136396 Show more Show less","Data Lineage, Data Modeling, Data Quality, Databricks, ETL, Pyspark, Python, SQL, Spark"
Data Engineer (Python),Infraveo Technologies,"Gujarat, India",https://in.linkedin.com/jobs/view/data-engineer-python-at-infraveo-technologies-4132470120,"This is a remote position. We are seeking a Data Engineer (Python) to join our team and help improve our tools, processes, and technical infrastructure. In this role, you will ensure the availability and quality of our data while contributing to the design, implementation, and maintenance of company products. You’ll collaborate across teams to create scalable and optimized software systems and play a key role in refining our development processes. This position offers a unique opportunity to learn and leverage cutting-edge AI development tools to enhance coding efficiency and develop advanced data tools and solutions. Responsibilities: Data Infrastructure: Design, implement, and maintain data pipelines, ensuring data availability, quality, and scalability. Product Development: Build and enhance products using Python, SQL, and modern technologies. AI Integration: Leverage cutting-edge AI tools, including Cursor, ChatGPT, and Claude to improve development processes, automate workflows, and create innovative data solutions. Collaboration: Communicate across departments to design, build, test, and implement new features and enhancements. Code Excellence: Produce quality, testable, and maintainable code while participating in code reviews. Problem Solving: Debug complex issues, interact with stakeholders, and design viable solutions with clear trade-offs. Process Optimization: Contribute to improving development processes, including deployment, on-call systems, and sprint planning. Team Growth: Collaborate within the development team to help each other grow and continuously learn. Stakeholder: Engagement: Partner with analysts, scientists, and IT to ensure alignment and deliver data-driven solutions. Other duties as assigned/necessary. Requirements Technical Tools: Experience with in Python and/or SQL. Experience with Bash, Shell scripting, Big Query, Airflow, and is a plus. Programming: Cleaning data using code-based approach. AI Proficiency: Interest or experience in using AI tools for coding and development of data solutions. Soft Skills: Problem-solving, cross-team collaboration, data storytelling, and stakeholder engagement. Adaptability: Thrives in a fast-paced startup environment and embraces change. Proactivity: Self-directed, organized, and able to prioritize effectively. Collaboration: Ability to work effectively with cross-functional teams. Education & Experience: A bachelor’s degree is required, and a graduate degree is preferred. Relevant experience in data engineering, software development, STEM, or related fields. Strong experience with SQL and Python is highly desirable. Mindset: A passion for innovation, continuous improvement, and delivering high-quality results from messy data. Preferred Qualifications: Familiarity with cloud-based solutions (e.g., Google Cloud Platform, Azure). Experience with ETL processes and tools. Experience building data visualization reports and dashboards. Knowledge of modern data storage solutions and database management systems. Exposure to DevOps practices and tools for deployment and monitoring. Benefits Work Location: Remote 5 days working Show more Show less","Bash, Data Pipelines, ETL, Python, SQL, Scala"
Data Engineer,Baker Hughes,"Pune, Maharashtra, India",https://in.linkedin.com/jobs/view/data-engineer-at-baker-hughes-4136597326,"Are you passionate to work with the team to create state-of-the-art data and analytics driven solutions? Are you interested in driving business analytics to a new level of predictive analytics while leveraging big data tools and technologies. Join our Data Engineering Team! The Data Engineering team helps solve our customers' toughest challenges; making flights safer, power cheaper, and oil & gas production safer for people and the environment by leveraging data and analytics. The Lead Data Engineer will work with the team to create state-of-the-art data and analytics driven solutions, working across Baker Hughes to drive business analytics to a new level of predictive analytics while leveraging big data tools and technologies. Partner with the best As a Data Engineer, you will be part of a data engineering or cross-disciplinary team on commercially-facing development projects, typically involving large, complex data sets. These teams typically include statisticians, computer scientists, software developers, engineers, product managers, and end users, working in concert with partners in Baker Hughes business units. Potential application areas include remote monitoring and diagnostics across infrastructure and industrial sectors, financial portfolio risk assessment, and operations optimization. As a Data Engineer, you will be responsible for: Performing a variety of data loads & data transformations. Working knowledge of methods for parsing, formatting, & transforming data into units consistent with analytical needs. Demonstrating proficiency in implementation of logical/physical data models that support MDM best practices. Performing integration of multiple data source-formats into master data load. Having Proficiency in the use of at least one ETL tool. Having good communication skills - both oral and written Identifying downstream implications of data loads/migration (e.g., data quality, regulatory, etc). Demonstrating skills in collaborating with different stakeholders, influencing decisions and an ability to take strategic view Fuel your passion To be successful in this role you will: Have a Bachelors Degree with minimum of 2 years of working experience. Have hands on experience working with database technologies, including ETL tools including Databricks Workflows using Pyspark / Python, and an ability to learn new technologies. Have good proficiency in writing and optimizing SQL queries and working with databases. Translates analytics problems into data requirements. Understands logical and physical data models, big data storage architecture, data modeling methodologies, metadata management, master data management, data lineage & data profiling. Understands the technology landscape, up to date on current technology trends and new technology, brings new ideas to the team. Work in a way that works for you We recognize that everyone is different and that the way in which people want to work and deliver at their best is different for everyone too. In this role, we can offer the following flexible working patterns: Working remotely from home or any other work location Working with us Our people are at the heart of what we do at Baker Hughes. We know we are better when all of our people are developed, engaged and able to bring their whole authentic selves to work. We invest in the health and well-being of our workforce, train and reward talent and develop leaders at all levels to bring out the best in each other. Working for you Our inventions have revolutionized energy for over a century. But to keep going forward tomorrow, we know we have to push the boundaries today. We prioritize rewarding those who embrace change with a package that reflects how much we value their input. Join us, and you can expect: Contemporary work-life balance policies and wellbeing activities Comprehensive private medical care options Safety net of life insurance and disability programs Tailored financial programs Additional elected or voluntary benefits About Us: We are an energy technology company that provides solutions to energy and industrial customers worldwide. Built on a century of experience and conducting business in over 120 countries, our innovative technologies and services are taking energy forward – making it safer, cleaner and more efficient for people and the planet. Join Us: Are you seeking an opportunity to make a real difference in a company that values innovation and progress? Join us and become part of a team of people who will challenge and inspire you! Let’s come together and take energy forward. Baker Hughes Company is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law. R136396 Show more Show less","Data Lineage, Data Modeling, Data Quality, Databricks, ETL, Pyspark, Python, SQL, Spark"
Data Engineer,iO Associates - UK/EU,"Bengaluru, Karnataka, India",https://in.linkedin.com/jobs/view/data-engineer-at-io-associates-uk-eu-4126498543,"Data Engineer (AWS) 6 months (Extension or conversion to full-time possible) Remote role, But the candidates must reside in Bangalore, Chennai, Gurgaon, Pune, or Mumbai About the Role We seek an experienced Data Engineer (AWS) to join our client. In this role, you will design, implement, and maintain scalable data analytics solutions on the AWS platform. You'll work closely with cross-functional teams to optimise data pipelines, ensure data quality, and address complex business challenges. Skills & Qualifications Minimum 3+ years of experience with AWS data technologies (e.g., Redshift, Glue, Kinesis). Strong knowledge of ETL processes, data modelling, and data governance. Proficiency in Python and SQL for data manipulation. Familiarity with data security and compliance standards. AWS certifications (e.g., Solutions Architect Associate) are a plus. If you're ready to work on cutting-edge data solutions in a fast-paced environment, apply today! Show more Show less","Data Governance, Data Pipelines, Data Quality, ETL, Python, SQL, Scala"
Data Engineer,System Soft Technologies,India,https://in.linkedin.com/jobs/view/data-engineer-at-system-soft-technologies-4133450245,"Data Engineer 100% REMOTE Min required: Basic knowledge of SF - should prepare data template, and then migrate to accounts, leads, opportunities without having someone handhold them. Should be proficient in data management - specifically the application of data. Understanding Data types whether optional, required, mast look up, field data types. Should profile / do some health checks and do clean ups of data, relational DB. Knowledge of SQL and also proficiency in using data loader and work bench. SQL queries, SF reports and Dashboards. Interpret data, analyze results using statistical techniques and provide ongoing reports Develop and implement databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and quality Acquire data from primary or secondary data sources and maintain databases/data systems Identify, analyze, and interpret trends or patterns in complex data sets Filter and “clean” data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems Work with management to prioritize business and information needs Locate and define new process improvement opportunities Requirements Proven working experience as a Data Analyst or Business Data Analyst Technical expertise regarding data models, database design development, data mining and segmentation techniques Strong knowledge of and experience with reporting packages (Business Objects etc), databases (SQL etc), programming (XML, Javascript, or ETL frameworks) Show more Show less","ETL, Java, JavaScript, SQL"
Data Engineer - Data Engineering,Bristlecone,"Pune, Maharashtra, India",https://in.linkedin.com/jobs/view/data-engineer-data-engineering-at-bristlecone-4096087415,"Job Description Role : Junior Python Developer Role Description Role played will be that of a Python Developer. Will be a good team player. Will learn the project functionality quickly and start the development as soon as possible. Will be a continuous learner of technology Will share his learning regarding the project functionality with the team. Will have good client facing skills and win client confidence as soon as possible. Will learn to use third party libraries for developing different features. Will be participating in peer reviews. Qualifications And Experience Should have at least 1 year in Python development experience. Must have worked on at least 1 project as a Python developer. Technical Skills Should be well versed with data structures. Should be well versed with OOPs concepts. Should be well versed with reading and writing files. Should be well versed with Error and Exception handling. Should be aware of the following modules of Python Standard Library Operating System Interface File Wildcards Command Line Arguments String Pattern Matching Dates and Times Internet Access Should be aware of the basic Python coding standards. Should be aware of using at least 2-3 third party libraries e.g. Django, Flask, Pyramid Behavioural And Soft Skills Proactive Quick Learner Good analytical and programming skills Teamwork Customer Sensitivity Excellent verbal and written communication Responsibilities QUALIFICATIONS About Us ABOUT US Bristlecone is the leading provider of AI-powered application transformation services for the connected supply chain. We empower our customers with speed, visibility, automation, and resiliency – to thrive on change. Our transformative solutions in Digital Logistics, Cognitive Manufacturing, Autonomous Planning, Smart Procurement and Digitalization are positioned around key industry pillars and delivered through a comprehensive portfolio of services spanning digital strategy, design and build, and implementation across a range of technology platforms. Bristlecone is ranked among the top ten leaders in supply chain services by Gartner. We are headquartered in San Jose, California, with locations across North America, Europe and Asia, and over 2,500 consultants. Bristlecone is part of the $19.4 billion Mahindra Group. Equal Opportunity Employer Bristlecone is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status . Information Security Responsibilities Understand and adhere to Information Security policies, guidelines and procedure, practice them for protection of organizational data and Information System. Take part in information security training and act while handling information. Report all suspected security and policy breach to InfoSec team or appropriate authority (CISO). Understand and adhere to the additional information security responsibilities as part of the assigned job role. Show more Show less",Python
Data Analyst,Recro,India,https://in.linkedin.com/jobs/view/data-analyst-at-recro-4124983969,"Role: Data Analyst Experience: 5 + years Skills: Power BI, SQL, Python Budget: 17 LPA Responsibilities Technical Expertise: Develop and maintain advanced Python scripts to support data processing, automation, and analytics workflows. Ensure robust data lineage practices across systems to track data origins, movements, and transformations. Utilize SQL for efficient data querying, transformation, and management. Create insightful data visualizations to communicate complex information to stakeholders. Business Acumen and Problem-Solving: Understand and analyze business needs to deliver effective technical solutions. Employ a pragmatic approach to problem-solving, ensuring swift and high-quality execution. Stakeholder Management: Collaborate with cross-functional teams and lead discussions to align technical solutions with business goals. Manage stakeholder expectations and influence large organizations to adopt innovative practices. Show more Show less","Data Lineage, Power BI, Python, SQL"
Data Engineer,Baker Hughes,"Hyderabad, Telangana, India",https://in.linkedin.com/jobs/view/data-engineer-at-baker-hughes-4136594461,"Are you passionate to work with the team to create state-of-the-art data and analytics driven solutions? Are you interested in driving business analytics to a new level of predictive analytics while leveraging big data tools and technologies. Join our Data Engineering Team! The Data Engineering team helps solve our customers' toughest challenges; making flights safer, power cheaper, and oil & gas production safer for people and the environment by leveraging data and analytics. The Lead Data Engineer will work with the team to create state-of-the-art data and analytics driven solutions, working across Baker Hughes to drive business analytics to a new level of predictive analytics while leveraging big data tools and technologies. Partner with the best As a Data Engineer, you will be part of a data engineering or cross-disciplinary team on commercially-facing development projects, typically involving large, complex data sets. These teams typically include statisticians, computer scientists, software developers, engineers, product managers, and end users, working in concert with partners in Baker Hughes business units. Potential application areas include remote monitoring and diagnostics across infrastructure and industrial sectors, financial portfolio risk assessment, and operations optimization. As a Data Engineer, you will be responsible for: Performing a variety of data loads & data transformations. Working knowledge of methods for parsing, formatting, & transforming data into units consistent with analytical needs. Demonstrating proficiency in implementation of logical/physical data models that support MDM best practices. Performing integration of multiple data source-formats into master data load. Having Proficiency in the use of at least one ETL tool. Having good communication skills - both oral and written Identifying downstream implications of data loads/migration (e.g., data quality, regulatory, etc). Demonstrating skills in collaborating with different stakeholders, influencing decisions and an ability to take strategic view Fuel your passion To be successful in this role you will: Have a Bachelors Degree with minimum of 2 years of working experience. Have hands on experience working with database technologies, including ETL tools including Databricks Workflows using Pyspark / Python, and an ability to learn new technologies. Have good proficiency in writing and optimizing SQL queries and working with databases. Translates analytics problems into data requirements. Understands logical and physical data models, big data storage architecture, data modeling methodologies, metadata management, master data management, data lineage & data profiling. Understands the technology landscape, up to date on current technology trends and new technology, brings new ideas to the team. Work in a way that works for you We recognize that everyone is different and that the way in which people want to work and deliver at their best is different for everyone too. In this role, we can offer the following flexible working patterns: Working remotely from home or any other work location Working with us Our people are at the heart of what we do at Baker Hughes. We know we are better when all of our people are developed, engaged and able to bring their whole authentic selves to work. We invest in the health and well-being of our workforce, train and reward talent and develop leaders at all levels to bring out the best in each other. Working for you Our inventions have revolutionized energy for over a century. But to keep going forward tomorrow, we know we have to push the boundaries today. We prioritize rewarding those who embrace change with a package that reflects how much we value their input. Join us, and you can expect: Contemporary work-life balance policies and wellbeing activities Comprehensive private medical care options Safety net of life insurance and disability programs Tailored financial programs Additional elected or voluntary benefits About Us: We are an energy technology company that provides solutions to energy and industrial customers worldwide. Built on a century of experience and conducting business in over 120 countries, our innovative technologies and services are taking energy forward – making it safer, cleaner and more efficient for people and the planet. Join Us: Are you seeking an opportunity to make a real difference in a company that values innovation and progress? Join us and become part of a team of people who will challenge and inspire you! Let’s come together and take energy forward. Baker Hughes Company is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law. R136396 Show more Show less","Data Lineage, Data Modeling, Data Quality, Databricks, ETL, Pyspark, Python, SQL, Spark"
Dataengineer_Deepanshi_TTP,CodersBrain,India,https://in.linkedin.com/jobs/view/dataengineer-deepanshi-ttp-at-codersbrain-4037170875,"Greetings from Coders Brain Technology Pvt. Ltd. Coders Brain is a global leader in its services, digital, and business solutions that partners with its clients to simplify, strengthen, and transform their businesses. We ensure the highest levels of certainty and satisfaction through a deep-set commitment to our clients, comprehensive industry expertise, and a global network of innovation and delivery centers. Location: Remote Position: Permanent with Coders Brain Technology Pvt. Ltd. Experience:3+ Years Notice Period: Immediate or 15 days Job Description Responsible for comparing two warehouse data, generating data QC reports. Good to have : Apache Spark or equivalent big data ETL tools, data quality jobs and optimisation of jobs with streaming data. If you're interested for above requirement, Please Share The Below-mentioned Details Current CTC: Expected CTC: Notice period: Current Company: Current Location: Preferred Location: Total-experience: Relevant experience: Highest qualification: DOJ(If Offer in Hand from Other company): Offer in hand: Also, send your updated CV, ASAP Skills: etl,data warehouse,mysql Show more Show less","Data Quality, ETL, MySQL, SQL, Spark"
Data Engineer II,Precision AQ,India,https://in.linkedin.com/jobs/view/data-engineer-ii-at-precision-aq-4085409809,"Position Summary The Data Engineer II is responsible for designing, developing and ensuring on-going quality management of processes and workflows (Data Pipeline, Analytics Platform, Product Delivery) built on our technology stack to support Precision products and/or applications involving data management and analytics across a range of healthcare information. In this role, they will work closely with our Product Management and Delivery teams to understand the needs of our clients (internal and external), the value proposition of our products and design/implement scalable technical solutions to deliver value to our clients. They will work under the guidance of the Sr Data Engineer and other leads. They will actively manage identification and justification of new data/analytics or automation opportunities for the products they support. Essential functions of the job include but are not limited to: Ability to translate business requirements into technical requirements Partner with SMEs to develop ad-hoc data queries and support tools Manage and improve monthly data operations processes for multiple product lines Active contributor on DevOps platforms to systematically manage code-base Take ownership of previously built solutions and support future enhancements Carry out existing and build new data ETLs for data management Manage continuous integration / continuous deploy (CI/CD) pipelines for dashboards and datasets for the organization ensuring change management and version control for data assets Conduct risk-based code reviews for solutions authored by our development partners Work with the Product Team to implement new application enhancements and features Interact with end users, internal users, development vendors, data analysts, and other engagement team members. Carry out internal dev tasks as delegated Work on automating various data loading as well as data validation tasks Qualifications Bachelor’s degree in engineering (B.E.) or in Technology (B. Tech.). Degree in computer science or related fields preferred. Minimum Required Minimum 2 years’ experience designing and reviewing complex data ecosystems with focus on data analysis systems including data warehousing, reporting, and strategic analytics 1+ years’ experience with cloud-based infrastructure (AWS, Azure, et al.), cloud database management (e.g., Redshift) Working Experience with Stored procedures preferably for ETL automation Other Required Fluency with SQL (Procedures), data warehousing, data analysis and experience with cloud based technology Fluency with code versioning systems/ tools – Git, and experience with CI/CD setups, preferably Azure DevOps (or any other) Experience in Life Sciences Experience leading technical discussions such as technology reviews Experience working in an onshore-offshore model Preferred Experience with the US Healthcare system and its key data sources. Experience with US Pharmaceutical Commercialization including Market Access or Commercial Operations Experience with Redshift Experience with Python Experience with an ETL tool or an ETL setup Any data provided as a part of this application will be stored in accordance with our Privacy Policy. For CA applicants, please also refer to our CA Privacy Notice. Precision Medicine Group is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, age, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status or other characteristics protected by law. © 2020 Precision Medicine Group, LLC If you are an individual with a disability and require a reasonable accommodation to complete any part of the application process or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact Precision Medicine Group at QuestionForHR@precisionmedicinegrp.com. It has come to our attention that some individuals or organizations are reaching out to job seekers and posing as potential employers presenting enticing employment offers. We want to emphasize that these offers are not associated with our company and may be fraudulent in nature. Please note that our organization will not extend a job offer without prior communication with our recruiting team, hiring managers and a formal interview process. Show more Show less","Azure DevOps, Data Warehousing, ETL, Python, SQL, Scala"
Data Engineer,Lingaro,India,https://in.linkedin.com/jobs/view/data-engineer-at-lingaro-4133858713,"Job Title: Data Engineer (MS Fabric)-Lead Consultant About Lingaro: Lingaro Group is the end-to-end data services partner to global brands and enterprises. We lead our clients through their data journey, from strategy through development to operations and adoption, helping them to realize the full value of their data. Since 2008, Lingaro has been recognized by clients and global research and advisory firms for innovation, technology excellence, and the consistent delivery of highest-quality data services. Our commitment to data excellence has created an environment that attracts the brightest global data talent to our team. Requirements: A bachelor's or master's degree in Computer Science, Information Systems, or a related field is typically required. Work commercial experience as a Data Engineer or a similar role. Proficiency in MS Fabric, Azure Data Factory, Azure Synapse Analytics, Azure Databricks. Extensive knowledge of MS Fabric components: Lakehouse, OneLake, Data Pipelines, Real-Time Analytics, Data warehouse, Power BI Integration, Semantic Models, Spark Jobs, Notebooks and Realtime Analytics, Dataflow Gen1 and Gen2, KQL. Integrate Fabric capabilities for seamless data flow, governance, and collaboration across teams. Strong understanding of Delta Lake, Parquet, and distributed data systems. Strong programming skills in Python, PySpark, Scala and Spark SQL/TSQL for data transformations. Excellent knowledge on Source control / Version Control along with CICD is a plus. Strong experience in implementation and management of lake House using Databricks and Azure Tech stack (ADLS Gen2, ADF, Azure SQL). Proficiency in data integration techniques, ETL processes and data pipeline architectures. Solid understanding of data processing techniques such as batch processing, real-time streaming, and data integration. Proficiency in working with relational and non-relational databases such as MSSQL, MySQL, PostgreSQL or Cassandra. Knowledge of data warehousing concepts and technologies like Redshift, Snowflake, or BigQuery is beneficial. Good knowledge of data storage architectures, including delta lakes, data warehouses, or distributed file systems Proficient in data modeling techniques and database optimization. Knowledge of query optimization, indexing, and performance tuning is necessary for efficient data retrieval and processing. Understanding of data security best practices and experience implementing data governance policies. Familiarity with data privacy regulations and compliance standards is a plus. Strong problem-solving abilities to identify and resolve issues related to data processing, storage, or infrastructure. Analytical mindset to analyze and interpret complex datasets for meaningful insights. Experience in designing and creating integration and unit test will be beneficial. Excellent communication skills to effectively collaborate with cross-functional teams, including data scientists, analysts, and business stakeholders. Ability to convey technical concepts to non-technical stakeholders in a clear and concise manner. A passion for staying updated with emerging technologies and industry trends in the field of big data engineering. Willingness to learn and adapt to new tools and techniques to enhance data processing, storage, and analysis capabilities. Proficient in database management systems such as SQL (Big Query is a must), NoSQL. Candidate should be able to design, configure, and manage databases to ensure optimal performance and reliability. Experience with data integration tools and techniques, such as ETL and ELT Candidate should be able to integrate data from multiple sources and transform it into a format that is suitable for analysis. Nice to Have: Certifications are an advantage. DP-600 (Fabric Analytics Engineer Associate) certification DP-700 (Microsoft Certified: Fabric Data Engineer Associate) DP-203 (Azure Data Engineer Associate) certification Databricks Associate/ Spark Developer Associate certification. Tasks: You will be a part of the team accountable for design, model and development of whole GCP data ecosystem for one of our Client’s (Cloud Storage, Cloud Functions, BigQuery) Involvement throughout the whole process starting with the gathering, analyzing, modelling, and documenting business/technical requirements will be needed. The role will include direct contact with clients. Designing and implementing data processing systems using Microsoft Fabric, Azure Data Analytics, Databricks and other distributed frameworks like Hadoop, Spark, Snowflake, Airflow, or other similar technologies. This involves writing efficient and scalable code to process, transform, and clean large volumes of structured and unstructured data. Building data pipelines to ingest data from various sources such as databases, APIs, or streaming platforms. Integrating and transforming data to ensure its compatibility with the target data model or format. Designing and optimizing data storage architectures, with One lake, data lakes, data warehouses, Serverless and any distributed file systems. Implementing techniques like partitioning, compression, or indexing to optimize data storage and retrieval. Identifying and resolving bottlenecks, tuning queries, and implementing caching strategies to enhance data retrieval speed and overall system efficiency. Designing and implementing data models that support efficient data storage, retrieval, and analysis. Collaborating with data scientists and analysts to understand their requirements and provide them with well-structured and optimized data for analysis and modeling purposes. Utilizing frameworks like Spark to perform distributed computing tasks, such as parallel processing, distributed data processing, or machine learning algorithms Implementing security measures to protect sensitive data and ensuring compliance with data privacy regulations. Establishing data governance practices to maintain data integrity, quality, and consistency. Identifying and resolving issues related to data processing, storage, or infrastructure. Monitoring system performance, identifying anomalies, and conducting root cause analysis to ensure smooth and uninterrupted data operations. Collaborating with cross-functional teams including data scientists, analysts, and business stakeholders to understand their requirements and provide technical solutions. Communicating complex technical concepts to non-technical stakeholders in a clear and concise manner. Independence and responsibility for delivering a solution Ability to work under Agile and Scrum development methodologies Staying updated with emerging technologies, tools, and techniques in the field of big data engineering. Exploring and recommending new technologies to enhance data processing, storage, and analysis capabilities. Train and mentor junior data engineers, providing guidance and knowledge transfer. Why join us: Stable employment. On the market since 2008, 1300+ talents currently on board in 7 global sites. 100% remote. Flexibility regarding working hours. Full-time position Comprehensive online onboarding program with a “Buddy” from day 1. Cooperation with top-tier engineers and experts. Unlimited access to the Udemy learning platform from day 1. Certificate training programs. Lingarians earn 500+ technology certificates yearly. Upskilling support. Capability development programs, Competency Centers, knowledge sharing sessions, community webinars, 110+ training opportunities yearly. Grow as we grow as a company. 76% of our managers are internal promotions. A diverse, inclusive, and values-driven community. Autonomy to choose the way you work. We trust your ideas. Create our community together. Refer your friends to receive bonuses. Activities to support your well-being and health. Plenty of opportunities to donate to charities and support the environment. Show more Show less","Azure Data Factory, Azure Databricks, Azure Synapse, Batch Processing, BigQuery, Cassandra, Cloud Storage, Data Governance, Data Lake, Data Modeling, Data Pipelines, Data Warehousing, Databricks, Delta Lake, ELT, ETL, Fabric, Lakehouse, Machine Learning, Microsoft Fabric, MySQL, Partitioning, PostgreSQL, Power BI, Pyspark, Python, SQL, Scala, Snowflake, Spark"
Data Engineer (DataBricks & Scala),Concentrix,India,https://in.linkedin.com/jobs/view/data-engineer-databricks-scala-at-concentrix-4121938423,"Job Description: Experience: 4 Years - 10 Years Job Location: Hyderabad/ Bengaluru/ Chennai/ Kolkata/ Mumbai/ Pune Work Mode: Remote (May be Hybrid later) Notice Period: Immediate to 1 week We are seeking a skilled Databricks Data Engineer with expertise in Scala to design, implement, and maintain scalable data solutions on the Databricks platform. The ideal candidate will have a strong background in data engineering, distributed computing, and cloud technologies, with a focus on leveraging Databricks and Apache Spark for data processing and analytics. The project in which the person will be assigned is a data engineering effort to support one of clients with timeseries data migration. Responsibilities: Design and Implementation : Develop and maintain robust data pipelines using Databricks Scala, ensuring efficient data processing and transformation. Data Modeling : Create and optimize data models to support analytical workloads and reporting needs. Collaboration : Work closely with cross-functional teams, including analysts, and business stakeholders, to deliver data-driven solutions that meet business objectives. Performance Optimization : Monitor and enhance the performance of existing data pipelines and applications, implementing best practices for scalability and efficiency. ETL Development : Design and implement ETL processes to extract, transform, and load data from various sources into Databricks. Automation : Utilize DevOps practices to automate deployment processes and monitor data pipelines for reliability. Documentation : Maintain clear documentation of data architecture, processes, and workflows to ensure knowledge sharing within the team. Skills & Qualifications: Programming Languages: Proficiency in Scala is required; experience with Python or SQL is a plus. Databricks Experience: Hands-on experience with the Databricks platform and Apache Spark for big data processing . Cloud Platforms: Strong knowledge of Azure in relation to data services. Distributed Computing: Understanding of distributed computing principles and big data technologies. Data Warehousing: Familiarity with data warehousing concepts and tools. Timeseries DB: Knowledge of timeseries DB like influx db Columnar Data store: Knowledge of columnar datastore like Apache Druid or Imply Polaris Bachelor’s or Master’s degree in Computer Science, Engineering, or a related field. 4+ years of experience in data engineering roles with a focus on Databricks and Scala development. Possess a strong understanding of data handling processes and demonstrate proficiency in data migration What we Offer: Growth opportunities for career advancement, including mentorship programs and training initiatives. Competitive compensation package Comprehensive benefits package, including health insurance, retirement plans, and wellness programs. Employee recognition programs to celebrate achievements and contributions. Work-life balance initiatives, including flexible work arrangements and paid time off for personal and family needs. Interested candidate kindly apply or share your resume to deeksha.sharma@concentrix.com Show more Show less","Data Modeling, Data Pipelines, Data Warehousing, Databricks, Druid, ETL, Python, SQL, Scala, Spark"
Data Engineer,Baker Hughes,"Mumbai, Maharashtra, India",https://in.linkedin.com/jobs/view/data-engineer-at-baker-hughes-4136595408,"Are you passionate to work with the team to create state-of-the-art data and analytics driven solutions? Are you interested in driving business analytics to a new level of predictive analytics while leveraging big data tools and technologies. Join our Data Engineering Team! The Data Engineering team helps solve our customers' toughest challenges; making flights safer, power cheaper, and oil & gas production safer for people and the environment by leveraging data and analytics. The Lead Data Engineer will work with the team to create state-of-the-art data and analytics driven solutions, working across Baker Hughes to drive business analytics to a new level of predictive analytics while leveraging big data tools and technologies. Partner with the best As a Data Engineer, you will be part of a data engineering or cross-disciplinary team on commercially-facing development projects, typically involving large, complex data sets. These teams typically include statisticians, computer scientists, software developers, engineers, product managers, and end users, working in concert with partners in Baker Hughes business units. Potential application areas include remote monitoring and diagnostics across infrastructure and industrial sectors, financial portfolio risk assessment, and operations optimization. As a Data Engineer, you will be responsible for: Performing a variety of data loads & data transformations. Working knowledge of methods for parsing, formatting, & transforming data into units consistent with analytical needs. Demonstrating proficiency in implementation of logical/physical data models that support MDM best practices. Performing integration of multiple data source-formats into master data load. Having Proficiency in the use of at least one ETL tool. Having good communication skills - both oral and written Identifying downstream implications of data loads/migration (e.g., data quality, regulatory, etc). Demonstrating skills in collaborating with different stakeholders, influencing decisions and an ability to take strategic view Fuel your passion To be successful in this role you will: Have a Bachelors Degree with minimum of 2 years of working experience. Have hands on experience working with database technologies, including ETL tools including Databricks Workflows using Pyspark / Python, and an ability to learn new technologies. Have good proficiency in writing and optimizing SQL queries and working with databases. Translates analytics problems into data requirements. Understands logical and physical data models, big data storage architecture, data modeling methodologies, metadata management, master data management, data lineage & data profiling. Understands the technology landscape, up to date on current technology trends and new technology, brings new ideas to the team. Work in a way that works for you We recognize that everyone is different and that the way in which people want to work and deliver at their best is different for everyone too. In this role, we can offer the following flexible working patterns: Working remotely from home or any other work location Working with us Our people are at the heart of what we do at Baker Hughes. We know we are better when all of our people are developed, engaged and able to bring their whole authentic selves to work. We invest in the health and well-being of our workforce, train and reward talent and develop leaders at all levels to bring out the best in each other. Working for you Our inventions have revolutionized energy for over a century. But to keep going forward tomorrow, we know we have to push the boundaries today. We prioritize rewarding those who embrace change with a package that reflects how much we value their input. Join us, and you can expect: Contemporary work-life balance policies and wellbeing activities Comprehensive private medical care options Safety net of life insurance and disability programs Tailored financial programs Additional elected or voluntary benefits About Us: We are an energy technology company that provides solutions to energy and industrial customers worldwide. Built on a century of experience and conducting business in over 120 countries, our innovative technologies and services are taking energy forward – making it safer, cleaner and more efficient for people and the planet. Join Us: Are you seeking an opportunity to make a real difference in a company that values innovation and progress? Join us and become part of a team of people who will challenge and inspire you! Let’s come together and take energy forward. Baker Hughes Company is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law. R136396 Show more Show less","Data Lineage, Data Modeling, Data Quality, Databricks, ETL, Pyspark, Python, SQL, Spark"
Data Engineer II,Precision AQ,India,https://in.linkedin.com/jobs/view/data-engineer-ii-at-precision-aq-4114559843,"Position Summary The Data Engineer II is responsible for designing, developing and ensuring on-going quality management of processes and workflows (Data Pipeline, Analytics Platform, Product Delivery) built on our technology stack to support Precision products and/or applications involving data management and analytics across a range of healthcare information. In this role, they will work closely with our Product Management and Delivery teams to understand the needs of our clients (internal and external), the value proposition of our products and design/implement scalable technical solutions to deliver value to our clients. They will work under the guidance of the Sr Data Engineer and other leads. They will actively manage identification and justification of new data/analytics or automation opportunities for the products they support. Essential functions of the job include but are not limited to: Ability to translate business requirements into technical requirements Partner with SMEs to develop ad-hoc data queries and support tools Manage and improve monthly data operations processes for multiple product lines Active contributor on DevOps platforms to systematically manage code-base Take ownership of previously built solutions and support future enhancements Carry out existing and build new data ETLs for data management Manage continuous integration / continuous deploy (CI/CD) pipelines for dashboards and datasets for the organization ensuring change management and version control for data assets Conduct risk-based code reviews for solutions authored by our development partners Work with the Product Team to implement new application enhancements and features Interact with end users, internal users, development vendors, data analysts, and other engagement team members. Carry out internal dev tasks as delegated Work on automating various data loading as well as data validation tasks Qualifications Bachelor’s degree in engineering (B.E.) or in Technology (B. Tech.). Degree in computer science or related fields preferred. Minimum Required 2+ years’ experience with cloud-based infrastructure (AWS, Azure, et al.), cloud database management (e.g., Snowflake) Working Experience with Stored procedures preferably for ETL automation 1+ years of Data modelling and preferably being a data architect for complex data ecosystems Other Required Fluency with SQL (Procedures), data warehousing, data analysis and experience with cloud based technology Fluency with code versioning systems/ tools – Git, and experience with CI/CD setups, preferably Azure DevOps (or any other) Experience in Life Sciences Experience leading technical discussions such as technology reviews Experience working in an onshore-offshore model Preferred Experience with the US Healthcare system and its key data sources. Experience with US Pharmaceutical Commercialization including Market Access or Commercial Operations Experience with Redshift Experience with Python Experience with an ETL tool or an ETL setup Any data provided as a part of this application will be stored in accordance with our Privacy Policy. For CA applicants, please also refer to our CA Privacy Notice. Precision Medicine Group is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, age, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status or other characteristics protected by law. © 2020 Precision Medicine Group, LLC If you are an individual with a disability and require a reasonable accommodation to complete any part of the application process or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact Precision Medicine Group at QuestionForHR@precisionmedicinegrp.com. It has come to our attention that some individuals or organizations are reaching out to job seekers and posing as potential employers presenting enticing employment offers. We want to emphasize that these offers are not associated with our company and may be fraudulent in nature. Please note that our organization will not extend a job offer without prior communication with our recruiting team, hiring managers and a formal interview process. Show more Show less","Azure DevOps, Data Warehousing, ETL, Python, SQL, Scala, Snowflake"
Data Analyst,Leadenhall Analytics,India,https://in.linkedin.com/jobs/view/data-analyst-at-leadenhall-analytics-4138469055,"We are seeking a highly skilled and experienced Data Analyst to join our dynamic team in a remote capacity. The ideal candidate will have a strong background in data analysis, with expertise in Advanced Excel and VBA . This role involves transforming data into actionable insights that will drive business decisions. The Data Analyst will be responsible for creating automated reports, building Excel models, and supporting data-driven decision-making processes. Knowledge of SQL and Power BI would be advantageous. Responsibilities: Design, develop, and maintain Excel-based reports, dashboards, and models using advanced formulas, pivot tables, and VBA macros for automation. Automate data processing and reporting tasks using VBA , reducing manual workload and improving efficiency. Analyse data trends and patterns to provide insights that support business strategies and decision-making. Ensure data accuracy and integrity in all reporting and analysis. Collaborate with stakeholders to understand their needs and deliver tailored data-driven solutions. Support integration of Excel models with SQL databases and Power BI dashboards where applicable. Qualifications: 0-2 years of relevant work experience in data analysis or a related field. Strong proficiency in Advanced Excel (including pivot tables, VLOOKUP, INDEX/MATCH, array formulas, Power Query). Expertise in VBA for automating reports, creating macros, and optimizing Excel-based processes. Knowledge of SQL (querying and extracting data from databases) and Power BI for data visualization is advantageous. Strong analytical and problem-solving skills, with the ability to interpret and present complex data effectively. Bachelor’s degree in Computer Science, Information Technology, Data Science, Mathematics, Business Analytics, or a related field . Ability to work independently in a fully remote environment and manage time effectively. Strong communication skills, both written and verbal, with the ability to convey complex data in a clear and concise manner. We want to ensure that all applicants have a fair and equal chance, so we’re doing an initial assessment to minimize unconscious bias in our hiring process. Successful candidates will be invited to a job interview. Ready to join our team? Start by clicking the ""Apply"" link above. Show more Show less","Power BI, Ray, SQL"
Software Dev Engineer I,Swiggy,"Kharagpur-I, West Bengal, India",https://in.linkedin.com/jobs/view/software-dev-engineer-i-at-swiggy-4086476492,"Way of working - Remote : Employees will have the freedom to work remotely all through the year. These employees, who form a large majority, will come together in their base location for a week, once every quarter. Job Title: Software Dev Engineer I [Native iOS] Location: Remote first Tenure: 1- 3years About The Team & Role Swiggy is India’s leading on-demand delivery platform with a tech-first approach to logistics and a solution-first approach to consumer demands. With a presence in 500 cities across India, partnerships with hundreds of thousands of restaurants, an employee base of over 5000, and a 2 lakh+ strong independent fleet of Delivery Executives, we deliver unparalleled convenience driven by continuous innovation. Built on the back of robust ML technology and fuelled by terabytes of data processed every day, Swiggy offers a fast, seamless and reliable delivery experience for millions of customers across India. From starting out as a hyperlocal food delivery service in 2014 to becoming a logistics hub of excellence today, our capabilities result not only in lightning-fast delivery for customers but also in a productive and fulfilling experience for our employees. With Swiggy’s New Supply and the recent launches of Swiggy Instamart, Swiggy Genie, and Guiltfree, we are consistently making waves in the market, while continually growing the opportunities we offer our people. Position Overview We are looking for highly motivated individuals who can join our engineering team as SDE-1. As an iOS SDE-1 at Swiggy, you will play a crucial role in developing and enhancing our iOS mobile application, which millions of users rely on to order food, groceries, dine in and enjoy a seamless delivery experience. This opportunity offers you a chance to work closely with our talented team of iOS developers and gain valuable hands-on experience in the fast-paced world of app development. We are seeking a highly skilled iOS Software Development Engineer (SDE-1) to join our dynamic team. The ideal candidate should have a passion for mobile technology and a proven track record of delivering high-quality iOS applications. As an SDE-1, you will be responsible for designing, developing, and maintaining iOS applications that delight our users and exceed industry standards. What will you get to do here? Design and Architecture: Collaborate with cross-functional teams to make our app more scalable and robust. Architect solutions that adhere to best practices and promote code reusability. Development: Write clean, maintainable, reusable code in Swift/SwiftUI. Implement new features, enhancements, and bug fixes according to project requirements and timelines. Testing: Develop and execute comprehensive unit tests and integration tests to ensure the reliability and stability of our Consumer App. Implement automated testing frameworks and strategies to streamline the testing process. Performance Optimization: Identify performance bottlenecks and optimize iOS applications for speed, responsiveness, and resource efficiency. Conduct code reviews and performance profiling to maintain high performance standards. Documentation: Create technical documentation, including design documents, API specifications, and release notes. Document codebase changes, architecture decisions, and development processes to facilitate knowledge sharing and onboarding. Collaboration: Collaborate closely with product managers, designers, and other engineers to translate product requirements into technical solutions. Participate in Agile ceremonies, such as sprint planning, daily stand-ups, and retrospectives. Continuous Improvement: Stay updated on the latest trends, tools, and technologies in iOS development. Continuously improve development processes, coding standards, and software quality through innovation and experimentation. What qualities are we looking for? Bachelor's degree in Computer Science, Engineering, or related field (Master's degree preferred). 1+ years of professional experience in iOS application development. Proficiency in Swift programming languages. Strong understanding of iOS SDK, X-Code, and related development tools. Experience with iOS architecture components Solid understanding of software design principles, patterns, and best practices. Experience with RESTful APIs, JSON/Proto etc Familiarity with version control systems (e.g., Git) and continuous integration tools (e.g., Jenkins). Excellent problem-solving skills and attention to detail. Strong communication and collaboration skills. Ability to thrive in a fast-paced, dynamic environment and adapt to changing priorities. Knowledge and hands on experience of Kotlin Multiplatform will be cherry on the top. Visit our tech blogs to learn more about some of the challenges we deal with: Making Swiggy Buttery Smooth. A good mobile developer not only does… | by Agam Mahajan Insight into Swiggy’s new Multimedia Card | by Mayank Jha Build Time Optimizations (Xcode). As an iOS developer, we have… | by Dhruvil Patel | Swiggy Bytes — Tech Blog Handling multiple caches in App Designing the Swiggy app to be truly ‘accessible’ | Episode-3 | by Agam Mahajan We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, colour, religion, sex, disability status, or any other characteristic protected by the law. Show more Show less","Jenkins, Perl, Scala"
Data & Business Intelligence Engineer,Concentrix,"Bengaluru, Karnataka, India",https://in.linkedin.com/jobs/view/data-business-intelligence-engineer-at-concentrix-4117146632,"Job Title: Data & Business Intelligence Engineer Job Description We’re Concentrix. A new breed of tech company — Human-centered. Tech-powered. Intelligence-fueled. We create game-changing solutions across the enterprise, that help brands grow across the world and into the future. We are trusted by clients across all major sectors, from up-and-coming success stories to iconic Fortune Global 500 brands in over 70 countries spanning 6 continents. Our game-changers: Challenge Conventions Deliver outcomes unimagined Create experiences that go beyond WOW If this is you, we would love to discuss career opportunities with you. In our Concentrix Catalyst team, you will work with the engine that powers the experience design and engineering capabilities at Concentrix. A leading global solutions company that reimagines everything CX through strategy, talent, and technology. We combine human-centered design, powerful data, and strong tech to accelerate CX transformation at scale. You will be surrounded by the best in the world providing market leading technology and insights to modernize and simplify the customer experience. Within our professional services team, you will deliver strategic consulting, design, advisory services, market research, and contact center analytics that deliver insights to improve outcomes and value for our clients. Hence achieving our vision. Concentrix provides eligible employees with an opportunity to enroll in many benefit programs, generally including private medical plans, great compensation package, retirement savings plans, paid learning days, and flexible workplaces. Specific benefits plans will vary by country/region. We’re a remote-first company looking for the absolute best talent in the world. Experience the power of a game-changing career. Responsibilities Lead data-driven recommendations and insights to support strategic projects across Central Operations Lead cross-functional teams in specific initiatives; recognizes dependencies between teams; drive and clarify ownership of those dependencies Understand the business, track operational performance, provide insights and recommendations Support company-wide programs through ownership of analytical projects Establish strong working relationships with peers in other Global Operations teams and cross-functional partners (sales leadership, finance, sales operations, systems) to achieve objectives Build dashboards and automated reports, working proficiently with data visualization and querying/analysis tools (e.g., Tableau, SQL, Python) Own data tables, and work with stakeholders to ensure available data is accurate, consistent, and timely Own technical implementations and documentation associated with datasets and strive for 100% accuracy Comfortably identify and frame specific business problems; creates hypotheses, builds key analyses, and develops recommendations; gains alignment with stakeholders for implementation. Independently manage expectations from stakeholders; manages deadlines/timeframes for larger initiatives and projects with minimal guidance on prioritization or dependencies Preferred Qualifications: BS degree in a quantitative discipline (statistics, operations research, computer science, information engineering, engineering, applied mathematics, economics, business analytics, etc) 2+ years of experience working with data in a business setting, including data management, process execution, operations, reporting, or analytics. 4+ years of experience in Tableau/PowerBI and SQL 2+ years of experience in Python Exceptional analytical and technical skills, with clear attention to detail Ability to leverage numbers and insights to influence & drive sound decision making Location: IND BANGALORE-MANYA Work-at-Home Language Requirements: Time Type: Full time If you are a California resident, by submitting your information, you acknowledge that you have read and have access to the Job Applicant Privacy Notice for California Residents R1569579 Show more Show less","Power BI, Python, SQL, Tableau"
Data Engineer,Ollion,Pune/Pimpri-Chinchwad Area,https://in.linkedin.com/jobs/view/data-engineer-at-ollion-4073882166,"OUR STORY Let’s be honest: there are lots of people out there doing what we do. We’re just not convinced they’re doing it right. Businesses are hungry for innovation and opportunity, but not at the cost of their independence. At Ollion, we’ve connected companies and capabilities around the world to help ambitious organizations make the most of their transformation and leave the status quo in the dust. WORKING AT OLLION Innovation is risky. It demands bold steps and big questions, but that’s the price of making change. We’ve got our head in the cloud and two feet on the ground, channeling tech’s endless potential towards a single goal: making a world of difference. And we’re building a global team to do just that— a team capable of making game-changing breakthroughs without ever losing sight of the people it will impact. This is more than consulting. This is the change you can be. THE OLLION DIFFERENCE At Ollion, we’re all in on your independence. Our teams are seasoned. Our solutions are straightforward—sometimes even groundbreaking. And our engagements? Exactly as long as you want them to be. We deliver fresh thinking and hard-earned insight in a way that works for you and your customers, arming your organization with everything you need to make your transformation truly mean something. WORKING WITH OLLION (our clients’ experiences) Progress matters more than process. Our global team of cloud-native pros is all about creating new and better ways to work—not just by solving your tech challenges, but by using technology to solve your business challenges. We keep the formulas, frameworks, and ten-point plans to a minimum, tackling your most pressing problems with a proprietary mix of good-old-fashioned ingenuity and refreshing humanity. DIVERSITY AT OLLION One of our cultural keystones, ‘Find the angle’ recognizes that every individual has different aspirations, needs and brings a unique perspective. We value diversity, inclusion, and equity (DE&I) as core to our success. We believe that a diverse workforce brings together unique perspectives, experiences, and ideas, leading to innovation, creativity, and better outcomes for our clients and our organization. We are on a journey and are committed to building a workplace that celebrates and respects individuals from all backgrounds, including but not limited to race, ethnicity, gender, sexual orientation, age, disability, and cultural heritage. As our commitment to diversity and inclusion is reflected in our: Awareness and sensitisation programs: to create awareness and sensitisation. We encourage open dialogue, active listening, and mutual respect, creating a safe and supportive environment for everyone to contribute their unique perspectives and ideas. Dedicated efforts to building diverse teams: that leverage the strength of our differences to tackle complex challenges and drive innovation. By embracing diversity, we broaden our collective knowledge, enhance problem-solving capabilities, and unlock limitless potential for our employees. Job Description Job Overview The Data Engineer will be responsible for designing and building modern data platforms to support data-driven decision making. The Data Engineer will execute technical implementation of data engineering and visualization projects . The Data Engineer will help build a data and analytics consulting practice by taking part in recruiting efforts, creating technical collateral, and staying on top of technology trends with ongoing training and certifications. The entire consulting team will be responsible for building long-term strategic relationships with clients and participating in all aspects of project delivery Key Responsibilities Design, implement, and develop data pipelines to collect and process large amounts of data from various sources. Implement data storage solutions that are scalable, secure, and efficient, such as data warehouses and databases. Develop and implement data validation and testing processes to ensure that data is processed accurately and efficiently. Automate data collection, processing, and reporting processes to minimize manual work and improve efficiency. Create high quality documents to capture problem statements, requirements, solutions and designs. Contribute to the development of reusable, repeatable collateral for use across the practice. Implement data quality checks and validation processes to ensure the accuracy, consistency, and security of data. Monitor data pipelines and processes, troubleshoot issues, and implement solutions to prevent recurrence Maintain comprehensive documentation of data architectures, processes, and workflows to facilitate knowledge sharing and collaboration. Qualifications Key Requirements Must Have Bachelor's degree in Computer Science, Information Technology, or a related field. 3-5 years of experience in data engineering Excellent problem-solving, organization, debugging, and analytical skills. Ability to work independently and in a team environment. Excellent communication skills for effectively expressing ideas to team members and clients. Understanding of relational database concepts and SQL. Hands on experience in one RDBMS (MSSQL/MySQL/PostgreSQL) Knowledge of cloud computing platforms, such as Amazon Web Services (AWS), Google Cloud Platform (GCP), or Microsoft Azure Hands on Experience in Data processing frameworks such as Apache Spark/Beam/Flink Experience in building data pipelines using orchestration tools or equivalent cloud services like Apache Airflow or NiFi, as well as ETL tools such as Azure Data Factory, AWS Glue, Matillion, or similar platforms. Hands on Experience with any one of data warehousing solutions, such as Snowflake/ Google BigQuery, or Databricks Debug and optimize existing data infrastructure and processes as needed. Experience with at least one programming language, such as Python/ Java Nice To Have Experience building large-scale, high throughput, 24x7 data systems. Any Data engineering certifications on any of the clouds Experience with Visualization tools, such as Power BI, Looker, Tableau, and QuickSight. Experience in designing, implementing, and managing event-driven data pipelines using Kafka or similar cloud-native streaming services like AWS Kinesis, Azure Event Hubs, or Google Cloud Pub/Sub. Exposure to machine learning algorithms, AI, and/or LLM, with implementation in practice. Experience with legacy data systems (e.g. Hadoop, Informatica). Experience in Migrating Data Engineering workloads from on-prem to cloud or between clouds Experience in GCP Data services is preferred Additional Information BENEFITS & PERKS FOR WORKING AT OLLION Our employees multiply their potential because they have opportunities to: Create a lasting Impact , Learn and Grow professionally & personally, Experience great Culture , and Be your Whole Self ! Beyond an amazing, collaborative work environment, great people, and inspiring, innovative work, we have some great benefits and perks: Benchmarked, competitive, in-market total rewards package including (but not limited to): base salary & short-term incentive for all employees We are a virtualby-default, small but Global organization; ‘learn wherever, whenever’ frees our people from a rigid view of learning and growth Retirement planning (i.e. CPF, EPF, company-matched 401(k)) Globally, we build benefit plans that offer choices for whatever stage in life our employees are in and allow for flexibility as life happens.  Employees have access to a fully comprehensive benefits package to choose the medical, dental, and vision insurance plan that best fits their lives. In addition to great healthcare coverage, we also offer all employees mental health resources and additional wellness programs. Generous time off and leave allowances And more! Ollion is an equal opportunity employer. We celebrate diversity and we are committed to creating an inclusive environment for all employees. Ollion does not discriminate in employment on the basis of race, color, religion, sex (including pregnancy and gender identity), national origin, political affiliation, sexual orientation, marital status, disability, genetic information, age, membership in an employee organization, parental status, military service, or other non-merit factor. Show more Show less","AWS Glue, AWS Kinesis, Apache Airflow, Azure Data Factory, Azure Event Hub, Azure Event Hubs, BigQuery, Data Pipelines, Data Quality, Data Warehousing, Databricks, ETL, Flink, Informatica, Java, Looker, Machine Learning, Matillion, MySQL, PostgreSQL, Power BI, Python, SQL, Scala, Snowflake, Spark, Tableau"
